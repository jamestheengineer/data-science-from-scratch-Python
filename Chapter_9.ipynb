{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKh8EZGsIB6yAxsa0AiddE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O_o0ZUx31-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting data\n",
        "\n",
        "# You can pipe data using stdin and stdout\n",
        "\n",
        "# egrep.py\n",
        "import sys, re\n",
        "\n",
        "# sys.argv is the list of command-line arguments\n",
        "# sys.argv[0] is the name of the program itself\n",
        "# sys.argv[1] will be the regex specified at the command line\n",
        "regex = sys.argv[1]\n",
        "\n",
        "# for every line passed into the script\n",
        "for line in sys.stdin:\n",
        "  # if it matches the regex, write it to stdout\n",
        "  if re.search(regex, line):\n",
        "    sys.stdout.write(line)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW3_cdc34p2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# line_count.py\n",
        "count = 0\n",
        "for line in sys.stdin:\n",
        "  count += 1\n",
        "\n",
        "# print goes to sys.stdout\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TvNGc1Q4zjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If I were to break these files out, you could then pipe like:\n",
        "# type SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeNjirtQ44b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another potential script\n",
        "# most_common_words.py\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "# pass in number of words as first argument\n",
        "try:\n",
        "  num_words : int(sys.argv[1])\n",
        "except:\n",
        "  print(\"usage: most_common_words.py num_words\")\n",
        "  sys.exit(1) # nonzero exit code indicates error\n",
        "\n",
        "counter = Counter(word.lower()                      # lowercase words\n",
        "                  for line in sys.stdin\n",
        "                  for word in line.strip().split()  # split on spaces\n",
        "                  if word)                          # skip empty 'words'\n",
        "\n",
        "for word, count in counter.most_common(num_words):\n",
        "  sys.stdout.write(str(count))\n",
        "  sys.stdout.write(\"\\t\")\n",
        "  sys.stdout.write(word)\n",
        "  sys.stdout.write(\"\\n\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm4BuK8W5CB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then you could do\n",
        "# cat the_bible.txt | python most_common_words.py 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVWQKOI5AMHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading Files\n",
        "p = \"\"\"\n",
        "Some random text to write out\n",
        "# lets start some lines like this\n",
        "# and this\n",
        "\"\"\"\n",
        "text_file = open(\"text.txt\", \"w+\");text_file.write(p);text_file.close()\n",
        "\n",
        "# 'r' means read-only, it's assumed if you leave it out\n",
        "file_for_reading = open('text.txt', 'r')\n",
        "file_for_reading2 = open('text.txt')\n",
        "\n",
        "# 'w' if write -- will destroy the file if it already exists!\n",
        "file_for_writing = open('writing_file.txt', 'w')\n",
        "\n",
        "# 'a' is append -- for adding to the end of the file\n",
        "file_for_appending = open('appending_file.txt', 'a')\n",
        "\n",
        "# don't forget to close your files when you are done\n",
        "file_for_writing.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1g1_kN0vqB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python will auto close files if you use a with block\n",
        "# with open('text.txt') as f:\n",
        "#  data = function_that_get_data_from(f)\n",
        "\n",
        "# at this point f has already been closed, so don't try to use it\n",
        "# process(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeaRQCPzxA3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you need to read a whole file, you can iterate over the lines of the file\n",
        "starts_with_hash = 0\n",
        "\n",
        "with open('text.txt') as f:\n",
        "  for line in f:              # look at each line in the file\n",
        "    if re.match(\"^#\", line):  # user a regex to see if it starts with '#'\n",
        "      starts_with_hash += 1   # if it does, add 1 to the count\n",
        "\n",
        "print(starts_with_hash)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt_qqfNsyavA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's get some domain names (although you can trip this particular approach up)\n",
        "def get_domain(email_address: str) -> str:\n",
        "  \"\"\"Split on '@' and return the last piece\"\"\"\n",
        "  return email_address.lower().split(\"@\")[-1]\n",
        "\n",
        "# a couple of tests\n",
        "assert get_domain('joelgrus@gmail.com') == 'gmail.com'\n",
        "assert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "#with open('email_addresses.txt', 'r') as f:\n",
        "#  domain_counts = Counter(get_domain(line.strip())\n",
        "#                          for line in f\n",
        "#                          if \"@\" in line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjFqbHji3UeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delimited files are most common. Edge cases are tough when dealing with tabs\n",
        "# or spaces or commas, so you shouldn't try to parse these yourself\n",
        "\n",
        "# Stock prices\n",
        "stock_prices = \"\"\"6/20/2014\\tAA\\t90.91\n",
        "6/20/2014\\tMSFT\\t41.68\n",
        "6/20/2014\\tFB\\t64.50\n",
        "\"\"\"\n",
        "print(stock_prices)\n",
        "\n",
        "text_file = open(\"stock_prices.txt\", \"w+\");text_file.write(stock_prices);text_file.close()\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('stock_prices.txt') as f:\n",
        "  tab_reader = csv.reader(f, delimiter='\\t')\n",
        "  for row in tab_reader:\n",
        "    print(row)\n",
        "    date = row[0]\n",
        "    symbol = row[1]\n",
        "    closing_price = float(row[2])\n",
        "    #process(date, symbol, closing_price)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urrpM2TI8Ibx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can also use a DictReader\n",
        "with open('colon_delimted_stock_prices.txt') as f:\n",
        "  colon_reader = csv.DictReader(f, delimiter = ':')\n",
        "  for dict_row in colon_reader:\n",
        "    date = dict_row[\"date\"]\n",
        "    symbol = dict_row[\"symbol\"]\n",
        "    closing_price = float(dict_row[\"closing_price\"])\n",
        "    process(date, symbol, closing_price)\n",
        "\n",
        "# You can still use DictReader even if your data doesn't have headers by passing it the keys as a 'fieldnames' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nT-NNt6h4z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can write out data using csv.writer\n",
        "todays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5}\n",
        "\n",
        "with open('comma_delimited_stock_prices.txt', 'w') as f:\n",
        "  csv_writer = csv.writer(f, delimiter = ',')\n",
        "  for stock, price in todays_prices.items():\n",
        "    csv_writer.writerow([stock, price])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P50oI5Uh6SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scraping the web. We'll use a couple external packages (i.e., not from scratch)\n",
        "!pip install beautifulsoup4 requests html5lib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4wNsOUjjHfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/\"\n",
        "        \"joelgrus/data/master/getting-data.html\")\n",
        "print(url)\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "first_paragraph = soup.find('p')\n",
        "first_paragraph_text = soup.p.text\n",
        "first_paragraph_words = soup.p.text.split()\n",
        "print(first_paragraph_text, first_paragraph_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP4FP1AEoWqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can tag attributes by treating soup like a dict\n",
        "first_paragraph_id = soup.p['id']\n",
        "first_paragraph_id2 = soup.p.get('id')\n",
        "print(first_paragraph_id, first_paragraph_id2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qN-Zm84pgzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can multiple tags at once\n",
        "all_paragraphs = soup.find_all('p') # or just soup('p')\n",
        "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7q-zoSWqG3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's how you find classes\n",
        "important_paragraphs = soup('p', {'class' : 'important'})\n",
        "important_paragraphs2 = soup('p', 'important')\n",
        "important_paragraphs3 = [ p for p in soup('p')\n",
        "                          if 'important' in p.get('class', [])]\n",
        "print(important_paragraphs, important_paragraphs2, important_paragraphs3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grDWA-a7qmHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spans in divs. Warning: will return the same <span> multiple times if it sits\n",
        "# inside multiple <div>s. Be more clever if that is the case\n",
        "spans_inside_divs = [span\n",
        "                     for div in soup('div')\n",
        "                     for span in div('span')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTACSBlfrTFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example: Keeping tabs on Congress\n",
        "url = \"https://www.house.gov/representatives\"\n",
        "text = requests.get(url).text\n",
        "soup = BeautifulSoup(text, \"html5lib\")\n",
        "\n",
        "all_urls = [a['href']\n",
        "            for a in soup('a')\n",
        "            if a.has_attr('href')]\n",
        "print(len(all_urls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_rJsuz8xIvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Too many! Let's regex!\n",
        "import re\n",
        "\n",
        "# Must start with http:// or https://\n",
        "# Must end with .house.gov or .house.gov/\n",
        "regex = r\"^https?://.*\\.house\\.gov/?$\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-UDqUCLxld1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's write some tests!\n",
        "assert re.match(regex, \"http://joel.house.gov\")\n",
        "assert re.match(regex, \"https://joel.house.gov\")\n",
        "assert re.match(regex, \"http://joel.house.gov/\")\n",
        "assert re.match(regex, \"https://joel.house.gov/\")\n",
        "assert not re.match(regex, \"joel.house.gov\")\n",
        "assert not re.match(regex, \"http://joel.house.com\")\n",
        "assert not re.match(regex, \"https://joel.house.gov/biography\")\n",
        "\n",
        "# And now apply\n",
        "good_urls = [url for url in all_urls if re.match(regex, url)]\n",
        "\n",
        "print(len(good_urls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV-G8TXnyf1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lots of duplicates, so\n",
        "good_urls = list(set(good_urls))\n",
        "\n",
        "print(len(good_urls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSqqnsv2y65b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "html = requests.get('https://jayapal.house.gov').text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "# Use a set because the links might appear multiple times\n",
        "links = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n",
        "\n",
        "print(links) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wae-ROmy1YHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These seem like not relative links like the book says, but let's keep going\n",
        "from typing import Dict, Set\n",
        "\n",
        "press_releases: Dict[str, Set[str]] = {}\n",
        "\n",
        "for house_url in good_urls:\n",
        "  html = requests.get(house_url).text\n",
        "  soup = BeautifulSoup(html, 'html5lib')\n",
        "  pr_links = {a['href'] for a in soup('a') if 'press releases'\n",
        "                                         in a.text.lower()}\n",
        "  print(f\"{house_url}: {pr_links}\")\n",
        "  press_releases[house_url] = pr_links"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkzPNcxz2n29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's see what press releases mention 'data'\n",
        "def paragraph_mentions(text: str, keyword: str) -> bool:\n",
        "  \"\"\"\n",
        "  Returns True if a <p> inside the text mentions {keyword}\n",
        "  \"\"\"\n",
        "  soup = BeautifulSoup(text, 'html5lib')\n",
        "  paragraphs = [p.get_text() for p in soup('p')]\n",
        "\n",
        "  return any(keyword.lower() in paragraph.lower()\n",
        "              for paragraph in paragraphs)\n",
        "\n",
        "# Quick test\n",
        "text = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\n",
        "assert paragraph_mentions(text, \"twitter\") # is inside a <p>\n",
        "assert not paragraph_mentions(text, \"facebook\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJzGw2KF4_N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And now to process the data\n",
        "for house_url, pr_links in press_releases.items():\n",
        "  for pr_link in pr_links:\n",
        "    url = f\"{house_url}/{pr_link}\"\n",
        "    text = requests.get(url).text\n",
        "\n",
        "    if paragraph_mentions(text, 'data'):\n",
        "      print(f\"{house_url}\")\n",
        "      break # done with this house_url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMjVEoli7zZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using APIs\n",
        "\n",
        "# A lot of times we'll be parsing json into Python objects\n",
        "import json\n",
        "\n",
        "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
        "                   \"author\" : \"Joel Grus\",\n",
        "                   \"publicationYear\" : 2019,\n",
        "                   \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
        "      \n",
        "# parse the JSON to create a Python dict\n",
        "deserialized = json.loads(serialized)\n",
        "assert deserialized[\"publicationYear\"] == 2019\n",
        "assert \"data science\" in deserialized[\"topics\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ypM9e1Coiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using an unautheticated API. \n",
        "import requests, json\n",
        "\n",
        "github_user = \"jamestheengineer\"\n",
        "endpoint = f\"https://api.github.com/users/{github_user}/repos\"\n",
        "repos = json.loads(requests.get(endpoint).text)\n",
        "print(repos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GlWN4jEDLda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install python-dateutil\n",
        "\n",
        "from collections import Counter\n",
        "from dateutil.parser import parse\n",
        "\n",
        "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
        "month_counts = Counter(date.month for date in dates)\n",
        "weekday_counts = Counter(date.weekday() for date in dates)\n",
        "print(dates, month_counts, weekday_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jinkvtewD6j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_5_repos = sorted(repos,\n",
        "                      key=lambda r: r[\"pushed_at\"],\n",
        "                      reverse=True)[:5]\n",
        "last_5_languages = [repo[\"language\"]\n",
        "                    for repo in last_5_repos]\n",
        "\n",
        "print(last_5_repos, last_5_languages)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80EgRuOrEOKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}