{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_23.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSYxjZUtcQmigamyN/mpsd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BIMq_aHMWZV",
        "outputId": "d7393e48-a26c-4a5e-b7fb-07d05778f95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "%cd data-science-from-scratch-Python/\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'data-science-from-scratch-Python'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 630 (delta 14), reused 0 (delta 0), pack-reused 606\u001b[K\n",
            "Receiving objects: 100% (630/630), 1.64 MiB | 17.29 MiB/s, done.\n",
            "Resolving deltas: 100% (395/395), done.\n",
            "/content/data-science-from-scratch-Python\n",
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=1918ba5f4644e761d80c98d047c64d711abc936412bfaf175d8a0aede834a5cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKitV8nxGD3B"
      },
      "source": [
        "users_interests = [\n",
        "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
        "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
        "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
        "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
        "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
        "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
        "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
        "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
        "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
        "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
        "    [\"statistics\", \"R\", \"statsmodels\"],\n",
        "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
        "    [\"pandas\", \"R\", \"Python\"],\n",
        "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
        "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGmOBhEBGKGv",
        "outputId": "239366f6-66e5-472c-a4ac-ae970ad1634c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# One simple approach is to recommend what is popular\n",
        "from collections import Counter\n",
        "\n",
        "popular_interests = Counter(interest \n",
        "                            for user_interests in users_interests\n",
        "                            for interest in user_interests)\n",
        "print(popular_interests)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'Python': 4, 'R': 4, 'Big Data': 3, 'HBase': 3, 'Java': 3, 'statistics': 3, 'regression': 3, 'probability': 3, 'Hadoop': 2, 'Cassandra': 2, 'MongoDB': 2, 'Postgres': 2, 'scikit-learn': 2, 'statsmodels': 2, 'pandas': 2, 'machine learning': 2, 'libsvm': 2, 'C++': 2, 'neural networks': 2, 'deep learning': 2, 'artificial intelligence': 2, 'Spark': 1, 'Storm': 1, 'NoSQL': 1, 'scipy': 1, 'numpy': 1, 'decision trees': 1, 'Haskell': 1, 'programming languages': 1, 'mathematics': 1, 'theory': 1, 'Mahout': 1, 'MapReduce': 1, 'databases': 1, 'MySQL': 1, 'support vector machines': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQzJeflYGsfo",
        "outputId": "25e1851f-6300-4051-d2b5-5ad32b647780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# We can the just suggest the most popular itnerests that are not already in a user's list\n",
        "from typing import List, Tuple\n",
        "\n",
        "def most_popular_new_interests(\n",
        "        user_interests : List[str],\n",
        "        max_results: int = 5) -> List[Tuple[str, int]]:\n",
        "    suggestions = [(interest, frequency)\n",
        "                    for interest, frequency in popular_interests.most_common()\n",
        "                    if interest not in user_interests]\n",
        "    return suggestions[:max_results]\n",
        "\n",
        "print(most_popular_new_interests(users_interests[1]))\n",
        "print(most_popular_new_interests(users_interests[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Python', 4), ('R', 4), ('Big Data', 3), ('Java', 3), ('statistics', 3)]\n",
            "[('Python', 4), ('R', 4), ('statistics', 3), ('regression', 3), ('probability', 3)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlysjJvoIXQL",
        "outputId": "b04380dc-24cc-48ee-a419-aea1da3e8eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Collaborative filtering to find new interests based on similarity to others who have similar things to you\n",
        "unique_interests = sorted({interest\n",
        "                           for user_interests in users_interests\n",
        "                           for interest in user_interests})\n",
        "assert unique_interests[:6] == [\n",
        "                                'Big Data',\n",
        "                                'C++',\n",
        "                                'Cassandra',\n",
        "                                'HBase',\n",
        "                                'Hadoop',\n",
        "                                'Haskell',\n",
        "]\n",
        "print(unique_interests)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Big Data', 'C++', 'Cassandra', 'HBase', 'Hadoop', 'Haskell', 'Java', 'Mahout', 'MapReduce', 'MongoDB', 'MySQL', 'NoSQL', 'Postgres', 'Python', 'R', 'Spark', 'Storm', 'artificial intelligence', 'databases', 'decision trees', 'deep learning', 'libsvm', 'machine learning', 'mathematics', 'neural networks', 'numpy', 'pandas', 'probability', 'programming languages', 'regression', 'scikit-learn', 'scipy', 'statistics', 'statsmodels', 'support vector machines', 'theory']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnQKxr-3K2Gq",
        "outputId": "19585f24-0f86-4cec-b2aa-532f9ee97738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Next we produce an interest vector of 0s and 10s for each user\n",
        "def make_user_interest_vector(user_interests: List[str]) -> List[int]:\n",
        "  \"\"\"\n",
        "  Given a list of interests, produce a vector whose ith element is 1\n",
        "  if unique_interests[i] is in the list, 0 otherwise\n",
        "  \"\"\"\n",
        "  return [1 if interest in user_interests else 0\n",
        "          for interest in unique_interests]\n",
        "\n",
        "user_interest_vectors = [make_user_interest_vector(user_interests)\n",
        "                        for user_interests in users_interests]\n",
        "print(user_interest_vectors)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjYSBpdML2Dn",
        "outputId": "67149487-0b55-4ec1-9744-b4f55508d5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# Because we have a small dataset, it's no problem to compute the pairwise similarities\n",
        "from Chapter_21 import cosine_similarity\n",
        "\n",
        "user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j)\n",
        "                      for interest_vector_j in user_interest_vectors]\n",
        "                     for interest_vector_i in user_interest_vectors]\n",
        "\n",
        "assert 0.56 < user_similarities[0][9] < 0.58, \"several shared interests\"\n",
        "assert 0.18 < user_similarities[0][8] < 0.20, \"only one shared interest\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1e7a24d73ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j)\n\u001b[1;32m      5\u001b[0m                       for interest_vector_j in user_interest_vectors]\n\u001b[0;32m----> 6\u001b[0;31m                      for interest_vector_i in user_interest_vectors]\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;36m0.56\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0muser_similarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.58\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"several shared interests\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'user_interest_vectors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnq-blCkQQ-w",
        "outputId": "73c2215e-e212-4aaf-b0d4-69b5506df82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
        "  pairs = [(other_user_id, similarity) \n",
        "            for other_user_id, similarity in\n",
        "                enumerate(user_similarities[user_id])\n",
        "            if user_id != other_user_id and similarity > 0]\n",
        "  return sorted(pairs,\n",
        "                key = lambda pair: pair[-1],\n",
        "                reverse=True)\n",
        "  \n",
        "most_similar_users_to(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9, 0.5669467095138409),\n",
              " (1, 0.3380617018914066),\n",
              " (8, 0.1889822365046136),\n",
              " (13, 0.1690308509457033),\n",
              " (5, 0.1543033499620919)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRS0KqPzGdg4",
        "outputId": "80ce22b8-71ac-4fda-8d98-c4d10b831cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def user_based_suggestions(user_id: int,\n",
        "                           include_current_interests: bool = False):\n",
        "  # Sum up the similarities\n",
        "  suggestions: Dict[str, float] = defaultdict(float)\n",
        "  for other_user_id, similarity in most_similar_users_to(user_id):\n",
        "    for interest in users_interests[other_user_id]:\n",
        "      suggestions[interest] += similarity\n",
        "\n",
        "  # Convert them to a sorted list\n",
        "  suggestions = sorted(suggestions.items(),\n",
        "                       key=lambda pair: pair[-1], \n",
        "                       reverse=True)\n",
        "  # And (maybe) exclude already interests\n",
        "  if include_current_interests:\n",
        "    return suggestions\n",
        "  else:\n",
        "    return [(suggestion, weight)\n",
        "            for suggestion, weight in suggestions\n",
        "            if suggestion not in users_interests[user_id]]\n",
        "\n",
        "user_based_suggestions(0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('MapReduce', 0.5669467095138409),\n",
              " ('MongoDB', 0.50709255283711),\n",
              " ('Postgres', 0.50709255283711),\n",
              " ('NoSQL', 0.3380617018914066),\n",
              " ('neural networks', 0.1889822365046136),\n",
              " ('deep learning', 0.1889822365046136),\n",
              " ('artificial intelligence', 0.1889822365046136),\n",
              " ('databases', 0.1690308509457033),\n",
              " ('MySQL', 0.1690308509457033),\n",
              " ('Python', 0.1543033499620919),\n",
              " ('R', 0.1543033499620919),\n",
              " ('C++', 0.1543033499620919),\n",
              " ('Haskell', 0.1543033499620919),\n",
              " ('programming languages', 0.1543033499620919)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDVqdzpSIatv"
      },
      "source": [
        "# Item-based suggestions\n",
        "\n",
        "interest_user_matrix = [[user_interest_vector[j]\n",
        "                         for user_interest_vector in user_interest_vectors]\n",
        "                        for j, _ in enumerate(unique_interests)]\n",
        "\n",
        "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n",
        "                            for user_vector_j in interest_user_matrix]\n",
        "                         for user_vector_i in interest_user_matrix]\n",
        "\n",
        "def most_similar_interests_to(interest_id: int):\n",
        "  similarities = interest_similarities[interest_id]\n",
        "  pairs = [(unique_interests[other_interest_id], similarity)\n",
        "            for other_interest_id, similarity in enumerate(similarities)\n",
        "            if interest_id != other_interest_id and similarity > 0]\n",
        "\n",
        "  return sorted(pairs,\n",
        "                key=lambda pair: pair[-1],\n",
        "                reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHlsuLiLO8q2",
        "outputId": "f8a44f81-24b9-4052-d295-1b6e87e37cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "most_similar_interests_to(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hadoop', 0.8164965809277261),\n",
              " ('Java', 0.6666666666666666),\n",
              " ('MapReduce', 0.5773502691896258),\n",
              " ('Spark', 0.5773502691896258),\n",
              " ('Storm', 0.5773502691896258),\n",
              " ('Cassandra', 0.4082482904638631),\n",
              " ('artificial intelligence', 0.4082482904638631),\n",
              " ('deep learning', 0.4082482904638631),\n",
              " ('neural networks', 0.4082482904638631),\n",
              " ('HBase', 0.3333333333333333)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weW4dvkeQD-f",
        "outputId": "d28cef7f-4b3c-4e8c-abd2-6ff3b3baca5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def item_based_suggestions(user_id: int,\n",
        "                           include_current_interests: bool = False):\n",
        "  # Add up the similar interests\n",
        "  suggestions = defaultdict(float)\n",
        "  user_interest_vector = user_interest_vectors[user_id]\n",
        "  for interest_id, is_interested in enumerate(user_interest_vector):\n",
        "    if is_interested == 1:\n",
        "      similar_interests = most_similar_interests_to(interest_id)\n",
        "      for interest, similarity in similar_interests:\n",
        "        suggestions[interest] += similarity\n",
        "\n",
        "  # Sort them by weight\n",
        "  suggestions = sorted(suggestions.items(),\n",
        "                       key=lambda pair: pair[-1],\n",
        "                       reverse=True)\n",
        "  \n",
        "  if include_current_interests:\n",
        "    return suggestions\n",
        "  else:\n",
        "    return [(suggestion, weight)\n",
        "            for suggestion, weight in suggestions\n",
        "            if suggestion not in users_interests[user_id]]\n",
        "\n",
        "item_based_suggestions(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('MapReduce', 1.861807319565799),\n",
              " ('MongoDB', 1.3164965809277263),\n",
              " ('Postgres', 1.3164965809277263),\n",
              " ('NoSQL', 1.2844570503761732),\n",
              " ('MySQL', 0.5773502691896258),\n",
              " ('databases', 0.5773502691896258),\n",
              " ('Haskell', 0.5773502691896258),\n",
              " ('programming languages', 0.5773502691896258),\n",
              " ('artificial intelligence', 0.4082482904638631),\n",
              " ('deep learning', 0.4082482904638631),\n",
              " ('neural networks', 0.4082482904638631),\n",
              " ('C++', 0.4082482904638631),\n",
              " ('Python', 0.2886751345948129),\n",
              " ('R', 0.2886751345948129)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2gsVI0VUwjm",
        "outputId": "bd915d94-339b-4cf5-e978-af0f0f82bff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-24 18:40:12--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip’\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  16.2MB/s    in 0.3s    \n",
            "\n",
            "2020-10-24 18:40:12 (16.2 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Q6g9l_v_Gr",
        "outputId": "6d175395-7678-42be-d74b-95f1e541f525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!unzip ml-100k.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ml-100k.zip\n",
            "   creating: ml-100k/\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EabxJPiBwUGe"
      },
      "source": [
        "# This points to the current directory, modify if your files are elsewhere\n",
        "MOVIES = \"ml-100k/u.item\"\n",
        "RATINGS = \"ml-100k/u.data\"\n",
        "from typing import NamedTuple\n",
        "\n",
        "class Rating(NamedTuple):\n",
        "  user_id: str\n",
        "  movie_id: str\n",
        "  rating: float\n",
        "\n",
        "import csv\n",
        "# We specify this encoding to avoid a UnicodeDecodeError\n",
        "# See https://stackoverlow.com/a/53136168/1076346\n",
        "with open(MOVIES, encoding=\"iso-8859-1\") as f:\n",
        "  reader = csv.reader(f, delimiter=\"|\")\n",
        "  movies = {movie_id: title for movie_id, title, *_ in reader}\n",
        "\n",
        "# Create a list of [Rating]\n",
        "with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
        "  reader = csv.reader(f, delimiter=\"\\t\")\n",
        "  ratings = [Rating(user_id, movie_id, float(rating))\n",
        "              for user_id, movie_id, rating, _ in reader]\n",
        "\n",
        "assert len(movies) == 1682\n",
        "assert len(list({rating.user_id for rating in ratings})) == 943"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceprF2mTwtCE",
        "outputId": "a76e5710-02f9-4f3e-c3ea-f56b018f5255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Average ratings for Star Wars movies\n",
        "import re\n",
        "\n",
        "# Data structure for accumulating ratings by movie_id\n",
        "star_wars_ratings = {movie_id: []\n",
        "                     for movie_id, title in movies.items()\n",
        "                     if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
        "\n",
        "# Iterate over ratings, accumulating the Star Wars ones\n",
        "for rating in ratings:\n",
        "  if rating.movie_id in star_wars_ratings:\n",
        "    star_wars_ratings[rating.movie_id].append(rating.rating)\n",
        "    \n",
        " # Compute the average rating for each movie\n",
        "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n",
        "                for movie_id, title_ratings in star_wars_ratings.items()]\n",
        "\n",
        "# And then print them in order\n",
        "for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
        "  print(f\"{avg_rating:.2f} {movies[movie_id]}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.36 Star Wars (1977)\n",
            "4.20 Empire Strikes Back, The (1980)\n",
            "4.01 Return of the Jedi (1983)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex7SWvwRsYGf"
      },
      "source": [
        "# Lets come up with a model to predict these ratings\n",
        "import random\n",
        "random.seed(0)\n",
        "random.shuffle(ratings)\n",
        "\n",
        "split1 = int(len(ratings) * 0.7)\n",
        "split2 = int(len(ratings) * 0.85)\n",
        "\n",
        "train = ratings[:split1] # 70% of the data\n",
        "validation = ratings[split1:split2] # 15% of the data\n",
        "test = ratings[split2:] # 15% of the data\n",
        "\n",
        "# Baseling model to make sure we do better. Just use average rating\n",
        "avg_rating = sum(rating.rating for rating in train) / len(train)\n",
        "baseline_error = sum((rating.rating - avg_rating) ** 2\n",
        "                     for rating in test) / len(test)\n",
        "\n",
        "# THis is what we hope to do better than\n",
        "assert 1.26 < baseline_error < 1.27"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a6KbP6lu1aH",
        "outputId": "52c5d4e8-97c0-4755-dc79-d61215c4a72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from Chapter_19 import random_tensor\n",
        "\n",
        "EMBEDDING_DIM = 2\n",
        "\n",
        "# Find unique ids\n",
        "user_ids = {rating.user_id for rating in ratings}\n",
        "movie_ids = {rating.movie_id for rating in ratings}\n",
        "\n",
        "# Then create a random vector per id\n",
        "user_vectors = {user_id: random_tensor(EMBEDDING_DIM)\n",
        "                for user_id in user_ids}\n",
        "movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n",
        "                for movie_id in movie_ids}\n",
        "\n",
        "from typing import List\n",
        "import tqdm\n",
        "from Chapter_04 import dot\n",
        "\n",
        "def loop(dataset: List[Rating],\n",
        "         learning_rate: float = None) -> None:\n",
        "    with tqdm.tqdm(dataset) as t:\n",
        "      loss = 0.0\n",
        "      for i, rating in enumerate(t):\n",
        "        movie_vector = movie_vectors[rating.movie_id]\n",
        "        user_vector = user_vectors[rating.user_id]\n",
        "        predicted = dot(user_vector, movie_vector)\n",
        "        error = predicted - rating.rating\n",
        "        loss += error ** 2\n",
        "\n",
        "        if learning_rate is not None:\n",
        "          user_gradient = [error * m_j for m_j in movie_vector]\n",
        "          movie_gradient = [error * u_j for u_j in user_vector]\n",
        "\n",
        "          # Take gradient steps\n",
        "          for j in range(EMBEDDING_DIM):\n",
        "            user_vector[j] -= learning_rate * user_gradient[j]\n",
        "            movie_vector[j] -= learning_rate * movie_gradient[j]\n",
        "        \n",
        "        t.set_description(f\"avg loss: {loss / (i + 1)}\")\n",
        "\n",
        "# Now let's train\n",
        "learning_rate = 0.05\n",
        "for epoch in range(20): \n",
        "  learning_rate *= 0.9\n",
        "  print(epoch, learning_rate)\n",
        "  loop(train, learning_rate=learning_rate)\n",
        "  loop(validation)\n",
        "loop(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg loss: 10.585401515182665:   0%|          | 81/70000 [00:00<01:26, 807.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.045000000000000005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "avg loss: 1.0739841503021723: 100%|██████████| 15000/15000 [00:15<00:00, 998.24it/s]\n",
            "avg loss: 1.0011008506477679:   0%|          | 236/70000 [00:00<01:11, 982.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 0.04050000000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "avg loss: 1.0063025483263188: 100%|██████████| 15000/15000 [00:15<00:00, 963.12it/s]\n",
            "avg loss: 0.9274645974282703:   0%|          | 215/70000 [00:00<01:15, 930.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3 0.03280500000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "avg loss: 0.9794856756854476:  31%|███▏      | 4704/15000 [00:05<00:12, 821.73it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCvC_QlP7I1p"
      },
      "source": [
        "from Chapter_10 import pca, transform\n",
        "\n",
        "original_vectors = [vector for vector in movie_vectors.values()]\n",
        "components = pca(original_vectors, 2)\n",
        "\n",
        "ratings_by_movie = defaultdict(list)\n",
        "for rating in ratings:\n",
        "  ratings_by_movie[rating.movie_id].append(rating.rating)\n",
        "\n",
        "vectors = [\n",
        "    (movid_id, \n",
        "     sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]),\n",
        "     movies[movie_id],\n",
        "     vector)\n",
        "    for movie_id, vector in zip(movie_vectors.keys(),\n",
        "                                transform(original_vectors, components))\n",
        "]\n",
        "\n",
        "# Print top 25 and bottom 25 by first principal component\n",
        "print(sorted(vectors, key=lambda v: v[-1][0])[:25])\n",
        "print(sorted(vectors, key=lambda v: v[-1][0])[-25])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}