{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_23.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMA510UtyKAgw4AMoKqbfqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BIMq_aHMWZV"
      },
      "source": [
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "%cd data-science-from-scratch-Python/\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKitV8nxGD3B"
      },
      "source": [
        "users_interests = [\n",
        "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
        "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
        "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
        "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
        "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
        "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
        "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
        "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
        "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
        "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
        "    [\"statistics\", \"R\", \"statsmodels\"],\n",
        "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
        "    [\"pandas\", \"R\", \"Python\"],\n",
        "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
        "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGmOBhEBGKGv"
      },
      "source": [
        "# One simple approach is to recommend what is popular\n",
        "from collections import Counter\n",
        "\n",
        "popular_interests = Counter(interest \n",
        "                            for user_interests in users_interests\n",
        "                            for interest in user_interests)\n",
        "print(popular_interests)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQzJeflYGsfo"
      },
      "source": [
        "# We can the just suggest the most popular itnerests that are not already in a user's list\n",
        "from typing import List, Tuple\n",
        "\n",
        "def most_popular_new_interests(\n",
        "        user_interests : List[str],\n",
        "        max_results: int = 5) -> List[Tuple[str, int]]:\n",
        "    suggestions = [(interest, frequency)\n",
        "                    for interest, frequency in popular_interests.most_common()\n",
        "                    if interest not in user_interests]\n",
        "    return suggestions[:max_results]\n",
        "\n",
        "print(most_popular_new_interests(users_interests[1]))\n",
        "print(most_popular_new_interests(users_interests[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlysjJvoIXQL"
      },
      "source": [
        "# Collaborative filtering to find new interests based on similarity to others who have similar things to you\n",
        "unique_interests = sorted({interest\n",
        "                           for user_interests in users_interests\n",
        "                           for interest in user_interests})\n",
        "assert unique_interests[:6] == [\n",
        "                                'Big Data',\n",
        "                                'C++',\n",
        "                                'Cassandra',\n",
        "                                'HBase',\n",
        "                                'Hadoop',\n",
        "                                'Haskell',\n",
        "]\n",
        "print(unique_interests)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnQKxr-3K2Gq"
      },
      "source": [
        "# Next we produce an interest vector of 0s and 10s for each user\n",
        "def make_user_interest_vector(user_interests: List[str]) -> List[int]:\n",
        "  \"\"\"\n",
        "  Given a list of interests, produce a vector whose ith element is 1\n",
        "  if unique_interests[i] is in the list, 0 otherwise\n",
        "  \"\"\"\n",
        "  return [1 if interest in user_interests else 0\n",
        "          for interest in unique_interests]\n",
        "\n",
        "user_interest_vectors = [make_user_interest_vector(user_interests)\n",
        "                        for user_interests in users_interests]\n",
        "print(user_interest_vectors)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjYSBpdML2Dn"
      },
      "source": [
        "# Because we have a small dataset, it's no problem to compute the pairwise similarities\n",
        "from Chapter_21 import cosine_similarity\n",
        "\n",
        "user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j)\n",
        "                      for interest_vector_j in user_interest_vectors]\n",
        "                     for interest_vector_i in user_interest_vectors]\n",
        "\n",
        "assert 0.56 < user_similarities[0][9] < 0.58, \"several shared interests\"\n",
        "assert 0.18 < user_similarities[0][8] < 0.20, \"only one shared interest\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnq-blCkQQ-w"
      },
      "source": [
        "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
        "  pairs = [(other_user_id, similarity) \n",
        "            for other_user_id, similarity in\n",
        "                enumerate(user_similarities[user_id])\n",
        "            if user_id != other_user_id and similarity > 0]\n",
        "  return sorted(pairs,\n",
        "                key = lambda pair: pair[-1],\n",
        "                reverse=True)\n",
        "  \n",
        "most_similar_users_to(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRS0KqPzGdg4"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def user_based_suggestions(user_id: int,\n",
        "                           include_current_interests: bool = False):\n",
        "  # Sum up the similarities\n",
        "  suggestions: Dict[str, float] = defaultdict(float)\n",
        "  for other_user_id, similarity in most_similar_users_to(user_id):\n",
        "    for interest in users_interests[other_user_id]:\n",
        "      suggestions[interest] += similarity\n",
        "\n",
        "  # Convert them to a sorted list\n",
        "  suggestions = sorted(suggestions.items(),\n",
        "                       key=lambda pair: pair[-1], \n",
        "                       reverse=True)\n",
        "  # And (maybe) exclude already interests\n",
        "  if include_current_interests:\n",
        "    return suggestions\n",
        "  else:\n",
        "    return [(suggestion, weight)\n",
        "            for suggestion, weight in suggestions\n",
        "            if suggestion not in users_interests[user_id]]\n",
        "\n",
        "user_based_suggestions(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDVqdzpSIatv"
      },
      "source": [
        "# Item-based suggestions\n",
        "\n",
        "interest_user_matrix = [[user_interest_vector[j]\n",
        "                         for user_interest_vector in user_interest_vectors]\n",
        "                        for j, _ in enumerate(unique_interests)]\n",
        "\n",
        "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n",
        "                            for user_vector_j in interest_user_matrix]\n",
        "                         for user_vector_i in interest_user_matrix]\n",
        "\n",
        "def most_similar_interests_to(interest_id: int):\n",
        "  similarities = interest_similarities[interest_id]\n",
        "  pairs = [(unique_interests[other_interest_id], similarity)\n",
        "            for other_interest_id, similarity in enumerate(similarities)\n",
        "            if interest_id != other_interest_id and similarity > 0]\n",
        "\n",
        "  return sorted(pairs,\n",
        "                key=lambda pair: pair[-1],\n",
        "                reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHlsuLiLO8q2"
      },
      "source": [
        "most_similar_interests_to(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weW4dvkeQD-f"
      },
      "source": [
        "def item_based_suggestions(user_id: int,\n",
        "                           include_current_interests: bool = False):\n",
        "  # Add up the similar interests\n",
        "  suggestions = defaultdict(float)\n",
        "  user_interest_vector = user_interest_vectors[user_id]\n",
        "  for interest_id, is_interested in enumerate(user_interest_vector):\n",
        "    if is_interested == 1:\n",
        "      similar_interests = most_similar_interests_to(interest_id)\n",
        "      for interest, similarity in similar_interests:\n",
        "        suggestions[interest] += similarity\n",
        "\n",
        "  # Sort them by weight\n",
        "  suggestions = sorted(suggestions.items(),\n",
        "                       key=lambda pair: pair[-1],\n",
        "                       reverse=True)\n",
        "  \n",
        "  if include_current_interests:\n",
        "    return suggestions\n",
        "  else:\n",
        "    return [(suggestion, weight)\n",
        "            for suggestion, weight in suggestions\n",
        "            if suggestion not in users_interests[user_id]]\n",
        "\n",
        "item_based_suggestions(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2gsVI0VUwjm"
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Q6g9l_v_Gr"
      },
      "source": [
        "!unzip ml-100k.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EabxJPiBwUGe"
      },
      "source": [
        "# This points to the current directory, modify if your files are elsewhere\n",
        "MOVIES = \"ml-100k/u.item\"\n",
        "RATINGS = \"ml-100k/u.data\"\n",
        "from typing import NamedTuple\n",
        "\n",
        "class Rating(NamedTuple):\n",
        "  user_id: str\n",
        "  movie_id: str\n",
        "  rating: float\n",
        "\n",
        "import csv\n",
        "# We specify this encoding to avoid a UnicodeDecodeError\n",
        "# See https://stackoverlow.com/a/53136168/1076346\n",
        "with open(MOVIES, encoding=\"iso-8859-1\") as f:\n",
        "  reader = csv.reader(f, delimiter=\"|\")\n",
        "  movies = {movie_id: title for movie_id, title, *_ in reader}\n",
        "\n",
        "# Create a list of [Rating]\n",
        "with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
        "  reader = csv.reader(f, delimiter=\"\\t\")\n",
        "  ratings = [Rating(user_id, movie_id, float(rating))\n",
        "              for user_id, movie_id, rating, _ in reader]\n",
        "\n",
        "assert len(movies) == 1682\n",
        "assert len(list({rating.user_id for rating in ratings})) == 943"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceprF2mTwtCE"
      },
      "source": [
        "# Average ratings for Star Wars movies\n",
        "import re\n",
        "\n",
        "# Data structure for accumulating ratings by movie_id\n",
        "star_wars_ratings = {movie_id: []\n",
        "                     for movie_id, title in movies.items()\n",
        "                     if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
        "\n",
        "# Iterate over ratings, accumulating the Star Wars ones\n",
        "for rating in ratings:\n",
        "  if rating.movie_id in star_wars_ratings:\n",
        "    star_wars_ratings[rating.movie_id].append(rating.rating)\n",
        "    \n",
        " # Compute the average rating for each movie\n",
        "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n",
        "                for movie_id, title_ratings in star_wars_ratings.items()]\n",
        "\n",
        "# And then print them in order\n",
        "for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
        "  print(f\"{avg_rating:.2f} {movies[movie_id]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex7SWvwRsYGf"
      },
      "source": [
        "# Lets come up with a model to predict these ratings\n",
        "import random\n",
        "random.seed(0)\n",
        "random.shuffle(ratings)\n",
        "\n",
        "split1 = int(len(ratings) * 0.7)\n",
        "split2 = int(len(ratings) * 0.85)\n",
        "\n",
        "train = ratings[:split1] # 70% of the data\n",
        "validation = ratings[split1:split2] # 15% of the data\n",
        "test = ratings[split2:] # 15% of the data\n",
        "\n",
        "# Baseling model to make sure we do better. Just use average rating\n",
        "avg_rating = sum(rating.rating for rating in train) / len(train)\n",
        "baseline_error = sum((rating.rating - avg_rating) ** 2\n",
        "                     for rating in test) / len(test)\n",
        "\n",
        "# THis is what we hope to do better than\n",
        "assert 1.26 < baseline_error < 1.27"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a6KbP6lu1aH"
      },
      "source": [
        "from Chapter_19 import random_tensor\n",
        "\n",
        "EMBEDDING_DIM = 2\n",
        "\n",
        "# Find unique ids\n",
        "user_ids = {rating.user_id for rating in ratings}\n",
        "movie_ids = {rating.movie_id for rating in ratings}\n",
        "\n",
        "# Then create a random vector per id\n",
        "user_vectors = {user_id: random_tensor(EMBEDDING_DIM)\n",
        "                for user_id in user_ids}\n",
        "movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n",
        "                for movie_id in movie_ids}\n",
        "\n",
        "from typing import List\n",
        "import tqdm\n",
        "from Chapter_04 import dot\n",
        "\n",
        "def loop(dataset: List[Rating],\n",
        "         learning_rate: float = None) -> None:\n",
        "    with tqdm.tqdm(dataset) as t:\n",
        "      loss = 0.0\n",
        "      for i, rating in enumerate(t):\n",
        "        movie_vector = movie_vectors[rating.movie_id]\n",
        "        user_vector = user_vectors[rating.user_id]\n",
        "        predicted = dot(user_vector, movie_vector)\n",
        "        error = predicted - rating.rating\n",
        "        loss += error ** 2\n",
        "\n",
        "        if learning_rate is not None:\n",
        "          user_gradient = [error * m_j for m_j in movie_vector]\n",
        "          movie_gradient = [error * u_j for u_j in user_vector]\n",
        "\n",
        "          # Take gradient steps\n",
        "          for j in range(EMBEDDING_DIM):\n",
        "            user_vector[j] -= learning_rate * user_gradient[j]\n",
        "            movie_vector[j] -= learning_rate * movie_gradient[j]\n",
        "        \n",
        "        t.set_description(f\"avg loss: {loss / (i + 1)}\")\n",
        "\n",
        "# Now let's train\n",
        "learning_rate = 0.05\n",
        "for epoch in range(20): \n",
        "  learning_rate *= 0.9\n",
        "  print(epoch, learning_rate)\n",
        "  loop(train, learning_rate=learning_rate)\n",
        "  loop(validation)\n",
        "loop(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCvC_QlP7I1p"
      },
      "source": [
        "from Chapter_10 import pca, transform\n",
        "\n",
        "original_vectors = [vector for vector in movie_vectors.values()]\n",
        "components = pca(original_vectors, 2)\n",
        "\n",
        "ratings_by_movie = defaultdict(list)\n",
        "for rating in ratings:\n",
        "  ratings_by_movie[rating.movie_id].append(rating.rating)\n",
        "\n",
        "vectors = [\n",
        "    (movie_id, \n",
        "     sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]),\n",
        "     movies[movie_id],\n",
        "     vector)\n",
        "    for movie_id, vector in zip(movie_vectors.keys(),\n",
        "                                transform(original_vectors, components))\n",
        "]\n",
        "\n",
        "# Print top 25 and bottom 25 by first principal component\n",
        "print(sorted(vectors, key=lambda v: v[-1][0])[:25])\n",
        "print(sorted(vectors, key=lambda v: v[-1][0])[-25])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez8wlWlDEQwf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}