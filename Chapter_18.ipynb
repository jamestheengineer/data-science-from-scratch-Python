{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOdp/LAqhEzlq7cPnLyHRQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alazT68gQvSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "4a874bd3-b471-4e0a-bf9b-36700aa04c3f"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "%cd data-science-from-scratch-Python/\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'data-science-from-scratch-Python'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 393 (delta 2), reused 0 (delta 0), pack-reused 387\u001b[K\n",
            "Receiving objects: 100% (393/393), 1.06 MiB | 1.18 MiB/s, done.\n",
            "Resolving deltas: 100% (242/242), done.\n",
            "/content/data-science-from-scratch-Python\n",
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=56fff1a8cf61df871bb51714f89666418c2cad74eadc17b2bafced0f7c249eba\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5cmZsztRQ1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bfd9d38-0c23-46e3-bb03-3835ed0db773"
      },
      "source": [
        "from Chapter_04 import Vector, dot\n",
        "\n",
        "def step_function(x: float) -> float:\n",
        "  return 1.0 if x >= 0 else 0.0\n",
        "\n",
        "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
        "  \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
        "  calculation = dot(weights, x) + bias\n",
        "  return step_function(calculation)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from Chapter_04.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvrpxOvdR_vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AND Gate\n",
        "and_weights = [2., 2]\n",
        "and_bias = -3.\n",
        "\n",
        "assert perceptron_output(and_weights, and_bias, [1,1]) == 1\n",
        "assert perceptron_output(and_weights, and_bias, [0,1]) == 0\n",
        "assert perceptron_output(and_weights, and_bias, [1,0]) == 0\n",
        "assert perceptron_output(and_weights, and_bias, [0,0]) == 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewYxTjeqfxhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OR Gate\n",
        "or_weights = [2., 2]\n",
        "or_bias = -1.\n",
        "\n",
        "assert perceptron_output(or_weights, or_bias, [1,1]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [1,0]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [0,1]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [0,0]) == 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9_hjkT6gdE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOT Gate\n",
        "not_weights = [-2.]\n",
        "not_bias = 1.\n",
        "\n",
        "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
        "assert perceptron_output(not_weights, not_bias, [1]) == 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ToIp5nhHo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XOR gate\n",
        "and_gate = min\n",
        "or_gate = max\n",
        "xor_gate = lambda x, y: 0 if x== y else 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PmNidyThXxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(t: float) -> float:\n",
        "  return 1 / (1 + math.exp(-t))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2ELxzl2uvjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
        "  # weights includes the bias term, inputs includes a 1\n",
        "  return sigmoid(dot(weights, inputs))\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYOGfnfWvAdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def feed_forward(neural_network: List[List[Vector]],\n",
        "                 input_vector: Vector) -> List[Vector]:\n",
        "    \"\"\"\n",
        "    Feeds the input vector through the neural network.\n",
        "    Returns the outputs of all layers (not just hte last one).\n",
        "    \"\"\"\n",
        "    outputs: List[Vector] = []\n",
        "\n",
        "    for layer in neural_network:\n",
        "      input_with_bias = input_vector + [1] # Add a constant\n",
        "      output = [neuron_output(neuron, input_with_bias) #Compute the output\n",
        "                for neuron in layer]\n",
        "      outputs.append(output)\n",
        "\n",
        "      # Then the input to the next layer is the output of this one\n",
        "      input_vector = output\n",
        "\n",
        "    return outputs\n",
        "\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUS-yh3CwIiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xor_network = [# hidden layer\n",
        "               [[20., 20, -30],   # 'and' neuron\n",
        "                [20., 20, -10]],  # 'or' neuron\n",
        "               # output layer\n",
        "               [[-60., 60, -30]]] # '2nd input but no 1st input' neuron\n",
        "\n",
        "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
        "# final output, and the [0] gets the value out of the resulting vector\n",
        "assert 0.000 < feed_forward(xor_network, [0,0])[-1][0] < 0.001\n",
        "assert 0.999 < feed_forward(xor_network, [1,0])[-1][0] < 1.000\n",
        "assert 0.999 < feed_forward(xor_network, [0,1])[-1][0] < 1.000\n",
        "assert 0.000 < feed_forward(xor_network, [1,1])[-1][0] < 0.001"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrIFwlFVAHJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}