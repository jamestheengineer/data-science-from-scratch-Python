{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2tQF9JvNbxjjUuJ4no6eQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alazT68gQvSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "%cd data-science-from-scratch-Python/\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5cmZsztRQ1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Chapter_04 import Vector, dot\n",
        "\n",
        "def step_function(x: float) -> float:\n",
        "  return 1.0 if x >= 0 else 0.0\n",
        "\n",
        "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
        "  \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
        "  calculation = dot(weights, x) + bias\n",
        "  return step_function(calculation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvrpxOvdR_vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AND Gate\n",
        "and_weights = [2., 2]\n",
        "and_bias = -3.\n",
        "\n",
        "assert perceptron_output(and_weights, and_bias, [1,1]) == 1\n",
        "assert perceptron_output(and_weights, and_bias, [0,1]) == 0\n",
        "assert perceptron_output(and_weights, and_bias, [1,0]) == 0\n",
        "assert perceptron_output(and_weights, and_bias, [0,0]) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewYxTjeqfxhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OR Gate\n",
        "or_weights = [2., 2]\n",
        "or_bias = -1.\n",
        "\n",
        "assert perceptron_output(or_weights, or_bias, [1,1]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [1,0]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [0,1]) == 1\n",
        "assert perceptron_output(or_weights, or_bias, [0,0]) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9_hjkT6gdE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOT Gate\n",
        "not_weights = [-2.]\n",
        "not_bias = 1.\n",
        "\n",
        "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
        "assert perceptron_output(not_weights, not_bias, [1]) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ToIp5nhHo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XOR gate\n",
        "and_gate = min\n",
        "or_gate = max\n",
        "xor_gate = lambda x, y: 0 if x== y else 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PmNidyThXxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(t: float) -> float:\n",
        "  return 1 / (1 + math.exp(-t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2ELxzl2uvjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
        "  # weights includes the bias term, inputs includes a 1\n",
        "  return sigmoid(dot(weights, inputs))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYOGfnfWvAdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def feed_forward(neural_network: List[List[Vector]],\n",
        "                 input_vector: Vector) -> List[Vector]:\n",
        "    \"\"\"\n",
        "    Feeds the input vector through the neural network.\n",
        "    Returns the outputs of all layers (not just hte last one).\n",
        "    \"\"\"\n",
        "    outputs: List[Vector] = []\n",
        "\n",
        "    for layer in neural_network:\n",
        "      input_with_bias = input_vector + [1] # Add a constant\n",
        "      output = [neuron_output(neuron, input_with_bias) #Compute the output\n",
        "                for neuron in layer]\n",
        "      outputs.append(output)\n",
        "\n",
        "      # Then the input to the next layer is the output of this one\n",
        "      input_vector = output\n",
        "\n",
        "    return outputs\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUS-yh3CwIiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xor_network = [# hidden layer\n",
        "               [[20., 20, -30],   # 'and' neuron\n",
        "                [20., 20, -10]],  # 'or' neuron\n",
        "               # output layer\n",
        "               [[-60., 60, -30]]] # '2nd input but no 1st input' neuron\n",
        "\n",
        "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
        "# final output, and the [0] gets the value out of the resulting vector\n",
        "assert 0.000 < feed_forward(xor_network, [0,0])[-1][0] < 0.001\n",
        "assert 0.999 < feed_forward(xor_network, [1,0])[-1][0] < 1.000\n",
        "assert 0.999 < feed_forward(xor_network, [0,1])[-1][0] < 1.000\n",
        "assert 0.000 < feed_forward(xor_network, [1,1])[-1][0] < 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrIFwlFVAHJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backpropagation\n",
        "def sqerror_gradients(network: List[List[Vector]],\n",
        "                      input_vector: Vector,\n",
        "                      target_vector: Vector) -> List[List[Vector]]:\n",
        "  \"\"\"\n",
        "  Given a neural network, an input vector, and a target vector,\n",
        "  make a prediction and compute the gradient of the squared error\n",
        "  loss with respect to the neuron weights.\n",
        "  \"\"\"\n",
        "  # forward pass\n",
        "  hidden_outputs, outputs = feed_forward(network, input_vector)\n",
        "\n",
        "  # gradients with respect to output neuron pre-activation outputs\n",
        "  output_deltas = [output * (1 - output) * (output - target)\n",
        "                    for output, target in zip(outputs, target_vector)]\n",
        "  \n",
        "  # gradients with respect to output neuron weights\n",
        "  output_grads = [[output_deltas[i] * hidden_output\n",
        "                   for hidden_output in hidden_outputs + [1]]\n",
        "                  for i, output_neuron in enumerate(network[-1])]\n",
        "\n",
        "  # gradients with respect to hidden neuron pre-activation outputs\n",
        "  hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
        "                   dot(output_deltas, [n[i] for n in network[-1]])\n",
        "                   for i, hidden_output in enumerate(hidden_outputs)]\n",
        "\n",
        "  # gradients with respect to hidden neuron weights\n",
        "  hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
        "                  for i, hidden_neuron in enumerate(network[0])]\n",
        "                  \n",
        "  return [hidden_grads, output_grads]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6B9BzetP8vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# training data\n",
        "xs = [[0., 0], [0., 1], [1.,0], [1.,1]]\n",
        "ys = [[0.], [1.], [1.], [0.]]\n",
        "\n",
        "# start with random weights\n",
        "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
        "           [[random.random() for _ in range(2 + 1)], # 1st hidden neuron\n",
        "            [random.random() for _ in range(2 + 1)]], # 2nd hidden neuron\n",
        "           # output layer: 2 inputs -> 1 output\n",
        "           [[random.random() for _ in range(2 + 1)]] # 1st output neuron\n",
        "          ]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rzPp_fv3zUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c6cc6a3e-979e-4ea2-9fa4-c2d5263cc965"
      },
      "source": [
        "from Chapter_08 import gradient_step\n",
        "import tqdm\n",
        "\n",
        "learning_rate = 1.0\n",
        "\n",
        "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
        "  for x, y in zip(xs,ys):\n",
        "    gradients = sqerror_gradients(network, x, y)\n",
        "\n",
        "    # Take a gradient step for each neuron in each layer\n",
        "    network = [[gradient_step(neuron, grad, -learning_rate)\n",
        "                for neuron, grad in zip(layer, layer_grad)]\n",
        "               for layer, layer_grad in zip(network, gradients)]\n",
        "\n",
        "# check that we learned xor\n",
        "assert feed_forward(network, [0,0])[-1][0] < 0.01\n",
        "assert feed_forward(network, [0,1])[-1][0] > 0.99\n",
        "assert feed_forward(network, [1,0])[-1][0] > 0.99\n",
        "assert feed_forward(network, [1,1])[-1][0] < 0.01\n",
        "print(network)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "neural net for xor: 100%|██████████| 20000/20000 [00:01<00:00, 11446.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[7.137127692932594, 7.136523106753526, -3.251038963015925], [5.322592618595284, 5.322277541116411, -8.151999000527617]], [[11.643129333608758, -12.30113897821493, -5.491927866215813]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft41Sc-D4zFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}