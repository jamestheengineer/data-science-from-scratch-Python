{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxZwu/s94kjyL4PYxyS+ts",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7a8s3fAKso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "%cd data-science-from-scratch-Python/\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bll3cxBdIujf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep learning chapter\n",
        "\n",
        "Tensor = list\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def shape(tensor: Tensor) -> List[int]:\n",
        "  sizes: List[int] = []\n",
        "  while isinstance(tensor, list):\n",
        "    sizes.append(len(tensor))\n",
        "    tensor = tensor[0]\n",
        "  return sizes\n",
        "\n",
        "assert shape([1, 2, 3]) == [3]\n",
        "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2In7rs_dJwae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_1d(tensor: Tensor) -> bool:\n",
        "  \"\"\"\n",
        "  If tensor[0] is a list, it's a higher-order tensor.\n",
        "  Otherwise, tensor is 1-dimensional (that is, a vector)\n",
        "  \"\"\"\n",
        "  return not isinstance(tensor[0], list)\n",
        "\n",
        "assert is_1d([1,2,3])\n",
        "assert not is_1d([[1,2],[3,4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiAIHUcUu8ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_sum(tensor: Tensor) -> float:\n",
        "  \"\"\"Sums up all the values in the tensor\"\"\"\n",
        "  if is_1d(tensor):\n",
        "    return sum(tensor) # just a list of floats, use Python sum\n",
        "  else:\n",
        "    return sum(tensor_sum(tensor_i) # Call tensor_sum on each row\n",
        "               for tensor_i in tensor) # and sum up those results\n",
        "\n",
        "assert tensor_sum([1,2,3]) == 6\n",
        "assert tensor_sum([[1,2],[3,4]]) == 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF97Cb4SvYXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Callable\n",
        "\n",
        "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
        "  \"\"\"Applies f elementwise\"\"\"\n",
        "  if is_1d(tensor):\n",
        "    return [f(x) for x in tensor]\n",
        "  else:\n",
        "    return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
        "\n",
        "assert tensor_apply(lambda x: x+1, [1, 2, 3]) == [2,3,4]\n",
        "assert tensor_apply(lambda x: 2 * x, [[1,2],[3,4]]) == [[2,4],[6,8]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFSLmJ4JwifU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zeros_like(tensor: Tensor) -> Tensor:\n",
        "  return tensor_apply(lambda _: 0.0, tensor)\n",
        "\n",
        "assert zeros_like([1,2,3]) == [0,0,0]\n",
        "assert zeros_like([[1,2],[3,4]]) == [[0,0],[0,0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvN1x2Gow_5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_combine(f: Callable[[float, float], float],\n",
        "                   t1: Tensor,\n",
        "                   t2: Tensor) -> Tensor:\n",
        "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
        "    if is_1d(t1):\n",
        "      return [f(x,y) for x, y in zip(t1,t2)]\n",
        "    else:\n",
        "      return [tensor_combine(f, t1_i, t2_i)\n",
        "              for t1_i, t2_i in zip(t1, t2)]\n",
        "\n",
        "import operator\n",
        "\n",
        "assert tensor_combine(operator.add, [1,2,3],[4,5,6]) == [5,7,9]\n",
        "assert tensor_combine(operator.mul, [1,2,3], [4,5,6]) == [4,10,18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sH1ExZIyHFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Iterable, Tuple\n",
        "\n",
        "class Layer:\n",
        "  \"\"\"\n",
        "  Our neural networks will be composed of layers, each of which\n",
        "  knows how to do some computation on its inputs in the \"forward\"\n",
        "  direction and propagate gradients in the \"backward\" direction.\n",
        "  \"\"\"\n",
        "  def backward(self, gradient):\n",
        "    \"\"\"\n",
        "    Similarly, we're not going to be prescriptive about what the\n",
        "    gradient looks like. It's up to you the user to make sure \n",
        "    that you're doing things sensibly.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def params(self) -> Iterable[Tensor]:\n",
        "    \"\"\"\n",
        "    Returns the parameters of this layer. The default implementation \n",
        "    returns nothing, so that if you have a layer with no parameters\n",
        "    you don't have to implement this.\n",
        "    \"\"\"\n",
        "    return ()\n",
        "\n",
        "  def grads(self) -> Iterable[Tensor]:\n",
        "    \"\"\"\n",
        "    Returns the gradients, in the smae order as params().\n",
        "    \"\"\"\n",
        "    return ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Uh-B4REbEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Chapter_18 import sigmoid\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Apply sigmoid to each element of the input tensor,\n",
        "    and save the results to use in backpropagation.\n",
        "    \"\"\"\n",
        "    self.sigmoids = tensor_apply(sigmoid, input)\n",
        "    return self.sigmoids\n",
        "\n",
        "  def backward(self, gradient: Tensor) -> Tensor:\n",
        "    return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
        "                          self.sigmoids,\n",
        "                          gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aywa2pqdAwUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to randomly generate our weight tensors\n",
        "import random\n",
        "\n",
        "from Chapter_06 import inverse_normal_cdf\n",
        "\n",
        "def random_uniform(*dims: int) -> Tensor:\n",
        "  if len(dims) == 1:\n",
        "    return [random.random() for _ in range(dims[0])]\n",
        "  else:\n",
        "    return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
        "\n",
        "def random_normal(*dims: int,\n",
        "                  mean: float = 1.0,\n",
        "                  variance: float = 1.0) -> Tensor:\n",
        "  if len(dims) == 1:\n",
        "    return [mean + variance * inverse_normal_cdf(random.random())\n",
        "            for _ in range(dims[0])]\n",
        "  else:\n",
        "    return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
        "            for _ in range(dims[0])]\n",
        "\n",
        "assert shape(random_uniform(2,3,4)) == [2,3,4]\n",
        "assert shape(random_normal(5,6,mean=10)) == [5,6]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irGD2UmzWmW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrap them all in a random_tensor function\n",
        "\n",
        "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
        "  if init == 'normal':\n",
        "    return random_normal(*dims)\n",
        "  elif init == 'uniform':\n",
        "    return random_uniform(*dims)\n",
        "  elif init == 'xavier':\n",
        "    variance = len(dims) / sum(dims)\n",
        "    return random_normal(*dims, variance=variance)\n",
        "  else:\n",
        "    raise ValueError(f\"unknown init: {init}\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxHZdBlSXq1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}