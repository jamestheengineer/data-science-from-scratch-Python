{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxTStLB6ied0myKz/9rR+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7a8s3fAKso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "%cd data-science-from-scratch-Python/\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bll3cxBdIujf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep learning chapter\n",
        "\n",
        "Tensor = list\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def shape(tensor: Tensor) -> List[int]:\n",
        "  sizes: List[int] = []\n",
        "  while isinstance(tensor, list):\n",
        "    sizes.append(len(tensor))\n",
        "    tensor = tensor[0]\n",
        "  return sizes\n",
        "\n",
        "assert shape([1, 2, 3]) == [3]\n",
        "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2In7rs_dJwae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_1d(tensor: Tensor) -> bool:\n",
        "  \"\"\"\n",
        "  If tensor[0] is a list, it's a higher-order tensor.\n",
        "  Otherwise, tensor is 1-dimensional (that is, a vector)\n",
        "  \"\"\"\n",
        "  return not isinstance(tensor[0], list)\n",
        "\n",
        "assert is_1d([1,2,3])\n",
        "assert not is_1d([[1,2],[3,4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiAIHUcUu8ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_sum(tensor: Tensor) -> float:\n",
        "  \"\"\"Sums up all the values in the tensor\"\"\"\n",
        "  if is_1d(tensor):\n",
        "    return sum(tensor) # just a list of floats, use Python sum\n",
        "  else:\n",
        "    return sum(tensor_sum(tensor_i) # Call tensor_sum on each row\n",
        "               for tensor_i in tensor) # and sum up those results\n",
        "\n",
        "assert tensor_sum([1,2,3]) == 6\n",
        "assert tensor_sum([[1,2],[3,4]]) == 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF97Cb4SvYXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Callable\n",
        "\n",
        "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
        "  \"\"\"Applies f elementwise\"\"\"\n",
        "  if is_1d(tensor):\n",
        "    return [f(x) for x in tensor]\n",
        "  else:\n",
        "    return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
        "\n",
        "assert tensor_apply(lambda x: x+1, [1, 2, 3]) == [2,3,4]\n",
        "assert tensor_apply(lambda x: 2 * x, [[1,2],[3,4]]) == [[2,4],[6,8]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFSLmJ4JwifU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zeros_like(tensor: Tensor) -> Tensor:\n",
        "  return tensor_apply(lambda _: 0.0, tensor)\n",
        "\n",
        "assert zeros_like([1,2,3]) == [0,0,0]\n",
        "assert zeros_like([[1,2],[3,4]]) == [[0,0],[0,0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvN1x2Gow_5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_combine(f: Callable[[float, float], float],\n",
        "                   t1: Tensor,\n",
        "                   t2: Tensor) -> Tensor:\n",
        "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
        "    if is_1d(t1):\n",
        "      return [f(x,y) for x, y in zip(t1,t2)]\n",
        "    else:\n",
        "      return [tensor_combine(f, t1_i, t2_i)\n",
        "              for t1_i, t2_i in zip(t1, t2)]\n",
        "\n",
        "import operator\n",
        "\n",
        "assert tensor_combine(operator.add, [1,2,3],[4,5,6]) == [5,7,9]\n",
        "assert tensor_combine(operator.mul, [1,2,3], [4,5,6]) == [4,10,18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sH1ExZIyHFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Iterable, Tuple\n",
        "\n",
        "class Layer:\n",
        "  \"\"\"\n",
        "  Our neural networks will be composed of layers, each of which\n",
        "  knows how to do some computation on its inputs in the \"forward\"\n",
        "  direction and propagate gradients in the \"backward\" direction.\n",
        "  \"\"\"\n",
        "  def backward(self, gradient):\n",
        "    \"\"\"\n",
        "    Similarly, we're not going to be prescriptive about what the\n",
        "    gradient looks like. It's up to you the user to make sure \n",
        "    that you're doing things sensibly.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def params(self) -> Iterable[Tensor]:\n",
        "    \"\"\"\n",
        "    Returns the parameters of this layer. The default implementation \n",
        "    returns nothing, so that if you have a layer with no parameters\n",
        "    you don't have to implement this.\n",
        "    \"\"\"\n",
        "    return ()\n",
        "\n",
        "  def grads(self) -> Iterable[Tensor]:\n",
        "    \"\"\"\n",
        "    Returns the gradients, in the smae order as params().\n",
        "    \"\"\"\n",
        "    return ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Uh-B4REbEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Chapter_18 import sigmoid\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Apply sigmoid to each element of the input tensor,\n",
        "    and save the results to use in backpropagation.\n",
        "    \"\"\"\n",
        "    self.sigmoids = tensor_apply(sigmoid, input)\n",
        "    return self.sigmoids\n",
        "\n",
        "  def backward(self, gradient: Tensor) -> Tensor:\n",
        "    return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
        "                          self.sigmoids,\n",
        "                          gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aywa2pqdAwUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to randomly generate our weight tensors\n",
        "import random\n",
        "\n",
        "from Chapter_06 import inverse_normal_cdf\n",
        "\n",
        "def random_uniform(*dims: int) -> Tensor:\n",
        "  if len(dims) == 1:\n",
        "    return [random.random() for _ in range(dims[0])]\n",
        "  else:\n",
        "    return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
        "\n",
        "def random_normal(*dims: int,\n",
        "                  mean: float = 1.0,\n",
        "                  variance: float = 1.0) -> Tensor:\n",
        "  if len(dims) == 1:\n",
        "    return [mean + variance * inverse_normal_cdf(random.random())\n",
        "            for _ in range(dims[0])]\n",
        "  else:\n",
        "    return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
        "            for _ in range(dims[0])]\n",
        "\n",
        "assert shape(random_uniform(2,3,4)) == [2,3,4]\n",
        "assert shape(random_normal(5,6,mean=10)) == [5,6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irGD2UmzWmW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrap them all in a random_tensor function\n",
        "\n",
        "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
        "  if init == 'normal':\n",
        "    return random_normal(*dims)\n",
        "  elif init == 'uniform':\n",
        "    return random_uniform(*dims)\n",
        "  elif init == 'xavier':\n",
        "    variance = len(dims) / sum(dims)\n",
        "    return random_normal(*dims, variance=variance)\n",
        "  else:\n",
        "    raise ValueError(f\"unknown init: {init}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxHZdBlSXq1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the linear layer\n",
        "from Chapter_04 import dot\n",
        "\n",
        "class Linear(Layer):\n",
        "  def __init__(self,\n",
        "               input_dim: int,\n",
        "               output_dim: int,\n",
        "               init: str = 'xavier') -> None:\n",
        "    \"\"\"\n",
        "    A layer of output_dim neurons, each with input_dim weights\n",
        "    (and a bias)\n",
        "    \"\"\"\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    # self.w[o] is the weights for the oth neuron\n",
        "    self.w = random_tensor(output_dim, input_dim, init=init)\n",
        "\n",
        "    # self.b[o] is the bias term for the oth neuron\n",
        "    self.b = random_tensor(output_dim, init=init)\n",
        "\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    # Save the input to use in the backward pass.\n",
        "    self.input = input\n",
        "    # Return the vector of neuron outputs.\n",
        "    return [dot(input, self.w[o]) + self.b[o]\n",
        "            for o in range(self.output_dim)]\n",
        "  \n",
        "  def backward(self, gradient: Tensor) -> Tensor:\n",
        "    # Each b[o] gets added to output[o], which means\n",
        "    # the gradient of b is the same as the output gradient.\n",
        "    self.b_grad = gradient\n",
        "\n",
        "    # Each w[o][i] multiples input[i] and gets added to outpu[o].\n",
        "    # So its gradient is input[i] * gradient[o].\n",
        "    self.w_grad = [[self.input[i] * gradient[o]\n",
        "                    for i in range(self.input_dim)]\n",
        "                   for o in range(self.output_dim)]\n",
        "    \n",
        "    # Each input[i] multiplies every w[o][i] and gets added to every\n",
        "    # output[o]. So its gradien is the sum of w[o][i] * gradient[o]\n",
        "    # across all the outputs.\n",
        "    return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
        "            for i in range(self.input_dim)]\n",
        "\n",
        "  def params(self) -> Iterable[Tensor]:\n",
        "    return [self.w, self.b]\n",
        "\n",
        "  def grads(self) -> Iterable[Tensor]:\n",
        "    return [self.w_grad, self.b_grad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meJVjeykOOrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a sequence of layers\n",
        "from typing import List\n",
        "\n",
        "class Sequential(Layer):\n",
        "  \"\"\"\n",
        "  A layer consisting of a sequence of other layers.\n",
        "  It's up to you to make sure that the output of each layer \n",
        "  makes sense as the input to the next layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, layers: List[Layer]) -> None:\n",
        "    self.layers = layers\n",
        "  \n",
        "  def forward(self, input):\n",
        "    \"\"\"Just forward the input through the layers in order.\"\"\"\n",
        "    for layer in self.layers:\n",
        "      input = layer.forward(input)\n",
        "      return input\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
        "    for layer in reversed(self.layers):\n",
        "      gradient = layer.backward(gradient)\n",
        "    return gradient\n",
        "  \n",
        "  def params(self) -> Iterable[Tensor]:\n",
        "    \"\"\"Just return the params from each layer.\"\"\"\n",
        "    return (param for layer in self.layers for param in layer.params())\n",
        "\n",
        "  def grad(self) -> Iterable[Tensor]:\n",
        "    \"\"\"Just return the grads from each layer.\"\"\"\n",
        "    return (grad for layer in self.layers for grad in layer.grads())\n",
        "\n",
        "# So we could represent the neural network we used for XOR as:\n",
        "xor_net = Sequential([\n",
        "                      Linear(input_dim=2, output_dim=2),\n",
        "                      Sigmoid(),\n",
        "                      Linear(input_dim=2, output_dim=1),\n",
        "                      Sigmoid()\n",
        "                      ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJF7Z1enxQzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# But we still need to train it, so more classes to write\n",
        "\n",
        "class Loss:\n",
        "  def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "    \"\"\"How good are out predictions? (Larger numbers are worse.)\"\"\"\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "    \"\"\"How does the loss change as the predictions change?\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "class SSE(Loss):\n",
        "  \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
        "  def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "    # Compute the tensor of squared differences\n",
        "    squared_errors = tensor_combine(\n",
        "                      lambda predicted, actual: (predicted - actual) ** 2,\n",
        "                      predicted,\n",
        "                      actual)\n",
        "    # And just add them up\n",
        "    return tensor_sum(squared_errors)\n",
        "\n",
        "  def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "    return tensor_combine(\n",
        "        lambda predicted, actual: 2 * (predicted - actual),\n",
        "        predicted,\n",
        "        actual)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcm3xqmz3QKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need an absract optimizer so we can create different variants of gradient descent\n",
        "\n",
        "class Optimizer:\n",
        "  \"\"\"\n",
        "  An optimizer updates the weights of a layer (in place) using information\n",
        "  known by either the layer or the optimizer (or by both).\n",
        "  \"\"\"\n",
        "  def step(self, layer: Layer) -> None:\n",
        "    raise NotImplementedError\n",
        "  \n",
        "class GradientDescent(Optimizer):\n",
        "  def __init__(self, learning_rate: float = 0.1) -> None:\n",
        "    self.lr = learning_rate\n",
        "  \n",
        "  def step(self, layer: Layer) -> None:\n",
        "    for param, grad in zip(layer.params(), layer.grads()):\n",
        "      # Update param using a gradient step\n",
        "      param[:] = tensor_combine(\n",
        "          lambda param, grad: param - grad * self.lr,\n",
        "          param,\n",
        "          grad)\n",
        "\n",
        "# Note the slice operator. You have to use this if you want to affect the original list. For example\n",
        "tensor = [[1,2],[3,4]]\n",
        "\n",
        "for row in tensor:\n",
        "  row = [0,0]\n",
        "assert tensor == [[1,2],[3,4]], \"assignment doesn't update a list\"\n",
        "\n",
        "for row in tensor:\n",
        "  row[:] = [0,0]\n",
        "assert tensor == [[0,0], [0,0]], \"but slice assignment does\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjm1GougwwY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's show the flexibility of this design by implmenting one more optimizer\n",
        "\n",
        "class Momentum(Optimizer):\n",
        "  def __init__(self,\n",
        "               learning_rate: float,\n",
        "               momentum: float = 0.9) -> None:\n",
        "    self.lr = learning_rate\n",
        "    self.mo = momentum\n",
        "    self.updates: List[Tensor] = [] # running average\n",
        "\n",
        "  def step(self, layer: Layer) -> None:\n",
        "    # If we have no previous updates, start with all zeroes\n",
        "    if not self.updates:\n",
        "      self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
        "    \n",
        "    for update, param, grad in zip(self.updates,\n",
        "                                   layer.params(),\n",
        "                                   layer.grad()):\n",
        "      # Apply momentum\n",
        "      update[:] = tensor_combine(\n",
        "          lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
        "          update,\n",
        "          grad)\n",
        "      \n",
        "      # Then take a gradient step\n",
        "      param[:] = tensor_combine(\n",
        "          lambda p, u: p - self.lr * u,\n",
        "          param,\n",
        "          update)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIfK4VQe2Cpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's revisit XOR\n",
        "\n",
        "# training data\n",
        "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
        "ys = [[0.], [1.], [1.], [0.]]\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "net = Sequential([\n",
        "                  Linear(input_dim=2, output_dim=2),\n",
        "                  Sigmoid(),\n",
        "                  Linear(input_dim=2, output_dim=1)\n",
        "                  ])\n",
        "\n",
        "import tqdm\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=0.1)\n",
        "loss = SSE()\n",
        "\n",
        "with tqdm.trange(3000) as t:\n",
        "  for epoch in t:\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for x, y in zip(xs, ys):\n",
        "        predicted = net.forward(x)\n",
        "        epoch_loss += loss.loss(predicted, y)\n",
        "        gradient = loss.gradient(predicted, y)\n",
        "        net.backward(gradient)\n",
        "\n",
        "        optimizer.step(net)\n",
        "\n",
        "    t.set_description(f\"xor loss {epoch_loss:.3f}\")\n",
        "    \n",
        "for param in net.params():\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvAHjZQD-cJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}