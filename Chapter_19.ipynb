{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP4+cf863KCc8zF4S770BHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7a8s3fAKso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "%cd data-science-from-scratch-Python/\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bll3cxBdIujf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep learning chapter\n",
        "\n",
        "Tensor = list\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def shape(tensor: Tensor) -> List[int]:\n",
        "  sizes: List[int] = []\n",
        "  while isinstance(tensor, list):\n",
        "    sizes.append(len(tensor))\n",
        "    tensor = tensor[0]\n",
        "  return sizes\n",
        "\n",
        "assert shape([1, 2, 3]) == [3]\n",
        "assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2In7rs_dJwae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_1d(tensor: Tensor) -> bool:\n",
        "  \"\"\"\n",
        "  If tensor[0] is a list, it's a higher-order tensor.\n",
        "  Otherwise, tensor is 1-dimensional (that is, a vector)\n",
        "  \"\"\"\n",
        "  return not isinstance(tensor[0], list)\n",
        "\n",
        "assert is_1d([1,2,3])\n",
        "assert not is_1d([[1,2],[3,4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiAIHUcUu8ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_sum(tensor: Tensor) -> float:\n",
        "  \"\"\"Sums up all the values in the tensor\"\"\"\n",
        "  if is_1d(tensor):\n",
        "    return sum(tensor) # just a list of floats, use Python sum\n",
        "  else:\n",
        "    return sum(tensor_sum(tensor_i) # Call tensor_sum on each row\n",
        "               for tensor_i in tensor) # and sum up those results\n",
        "\n",
        "assert tensor_sum([1,2,3]) == 6\n",
        "assert tensor_sum([[1,2],[3,4]]) == 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF97Cb4SvYXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Callable\n",
        "\n",
        "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
        "  \"\"\"Applies f elementwise\"\"\"\n",
        "  if is_1d(tensor):\n",
        "    return [f(x) for x in tensor]\n",
        "  else:\n",
        "    return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
        "\n",
        "assert tensor_apply(lambda x: x+1, [1, 2, 3]) == [2,3,4]\n",
        "assert tensor_apply(lambda x: 2 * x, [[1,2],[3,4]]) == [[2,4],[6,8]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFSLmJ4JwifU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zeros_like(tensor: Tensor) -> Tensor:\n",
        "  return tensor_apply(lambda _: 0.0, tensor)\n",
        "\n",
        "assert zeros_like([1,2,3]) == [0,0,0]\n",
        "assert zeros_like([[1,2],[3,4]]) == [[0,0],[0,0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvN1x2Gow_5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_combine(f: Callable[[float, float], float],\n",
        "                   t1: Tensor,\n",
        "                   t2: Tensor) -> Tensor:\n",
        "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
        "    if is_1d(t1):\n",
        "      return [f(x,y) for x, y in zip(t1,t2)]\n",
        "    else:\n",
        "      return [tensor_combine(f, t1_i, t2_i)\n",
        "              for t1_i, t2_i in zip(t1, t2)]\n",
        "\n",
        "import operator\n",
        "\n",
        "assert tensor_combine(operator.add, [1,2,3],[4,5,6]) == [5,7,9]\n",
        "assert tensor_combine(operator.mul, [1,2,3], [4,5,6]) == [4,10,18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sH1ExZIyHFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Iterable, Tuple\n",
        "\n",
        "class Layer:\n",
        "  \"\"\"\n",
        "  Our neural networks will be composed of layers, each of which\n",
        "  knows how to do some computation on its inputs in the \"forward\"\n",
        "  direction and propagate gradients in the \"backward\" direction.\n",
        "  \"\"\"\n",
        "  def backward(self, gradient):\n",
        "    \"\"\"\n",
        "    Similarly, we're not going to be prescriptive about what the\n",
        "    gradient looks like. It's up to you the user to make sure \n",
        "    that you're doing things sensibly.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def params(self) -> Iterable[Tensor]:\n",
        "    \"\"\"\n",
        "    Returns the parameters of this layer. The default implementation \n",
        "    returns nothing, so that if you have a layer with no parameters\n",
        "    you don't have to implement this.\n",
        "    \"\"\"\n",
        "    return ()\n",
        "\n",
        "  def grads(self) -> Iterable[Tensor]:\n",
        "    \"\"\"\n",
        "    Returns the gradients, in the smae order as params().\n",
        "    \"\"\"\n",
        "    return ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Uh-B4REbEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Chapter_18 import sigmoid\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Apply sigmoid to each element of the input tensor,\n",
        "    and save the results to use in backpropagation.\n",
        "    \"\"\"\n",
        "    self.sigmoids = tensor_apply(sigmoid, input)\n",
        "    return self.sigmoids\n",
        "\n",
        "  def backward(self, gradient: Tensor) -> Tensor:\n",
        "    return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
        "                          self.sigmoids,\n",
        "                          gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aywa2pqdAwUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to randomly generate our weight tensors\n",
        "import random\n",
        "\n",
        "from Chapter_06 import inverse_normal_cdf\n",
        "\n",
        "def random_uniform(*dims: int) -> Tensor:\n",
        "  if len(dims) == 1:\n",
        "    return [random.random() for _ in range(dims[0])]\n",
        "  else:\n",
        "    return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
        "\n",
        "def random_normal(*dims: int,\n",
        "                  mean: float = 1.0,\n",
        "                  variance: float = 1.0) -> Tensor:\n",
        "  if len(dims) == 1:\n",
        "    return [mean + variance * inverse_normal_cdf(random.random())\n",
        "            for _ in range(dims[0])]\n",
        "  else:\n",
        "    return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
        "            for _ in range(dims[0])]\n",
        "\n",
        "assert shape(random_uniform(2,3,4)) == [2,3,4]\n",
        "assert shape(random_normal(5,6,mean=10)) == [5,6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irGD2UmzWmW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrap them all in a random_tensor function\n",
        "\n",
        "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
        "  if init == 'normal':\n",
        "    return random_normal(*dims)\n",
        "  elif init == 'uniform':\n",
        "    return random_uniform(*dims)\n",
        "  elif init == 'xavier':\n",
        "    variance = len(dims) / sum(dims)\n",
        "    return random_normal(*dims, variance=variance)\n",
        "  else:\n",
        "    raise ValueError(f\"unknown init: {init}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxHZdBlSXq1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the linear layer\n",
        "from Chapter_04 import dot\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
        "        \"\"\"\n",
        "        A layer of output_dim neurons, each with input_dim weights\n",
        "        (and a bias).\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # self.w[o] is the weights for the o-th neuron\n",
        "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
        "\n",
        "        # self.b[o] is the bias term for the o-th neuron\n",
        "        self.b = random_tensor(output_dim, init=init)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        # Save the input to use in the backward pass.\n",
        "        self.input = input\n",
        "\n",
        "        # Return the vector of neuron outputs.\n",
        "        return [dot(input, self.w[o]) + self.b[o]\n",
        "                for o in range(self.output_dim)]\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        # Each b[o] gets added to output[o], which means\n",
        "        # the gradient of b is the same as the output gradient.\n",
        "        self.b_grad = gradient\n",
        "\n",
        "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
        "        # So its gradient is input[i] * gradient[o].\n",
        "        self.w_grad = [[self.input[i] * gradient[o]\n",
        "                        for i in range(self.input_dim)]\n",
        "                       for o in range(self.output_dim)]\n",
        "\n",
        "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
        "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
        "        # across all the outputs.\n",
        "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
        "                for i in range(self.input_dim)]\n",
        "\n",
        "    def params(self) -> Iterable[Tensor]:\n",
        "        return [self.w, self.b]\n",
        "\n",
        "    def grads(self) -> Iterable[Tensor]:\n",
        "        return [self.w_grad, self.b_grad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meJVjeykOOrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a sequence of layers\n",
        "from typing import List\n",
        "\n",
        "class Sequential(Layer):\n",
        "    \"\"\"\n",
        "    A layer consisting of a sequence of other layers.\n",
        "    It's up to you to make sure that the output of each layer\n",
        "    makes sense as the input to the next layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers: List[Layer]) -> None:\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
        "        for layer in reversed(self.layers):\n",
        "            gradient = layer.backward(gradient)\n",
        "        return gradient\n",
        "\n",
        "    def params(self) -> Iterable[Tensor]:\n",
        "        \"\"\"Just return the params from each layer.\"\"\"\n",
        "        return (param for layer in self.layers for param in layer.params())\n",
        "\n",
        "    def grads(self) -> Iterable[Tensor]:\n",
        "        \"\"\"Just return the grads from each layer.\"\"\"\n",
        "        return (grad for layer in self.layers for grad in layer.grads())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJF7Z1enxQzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# But we still need to train it, so more classes to write\n",
        "\n",
        "class Loss:\n",
        "  def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "    \"\"\"How good are out predictions? (Larger numbers are worse.)\"\"\"\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "    \"\"\"How does the loss change as the predictions change?\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "class SSE(Loss):\n",
        "  \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
        "  def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "    # Compute the tensor of squared differences\n",
        "    squared_errors = tensor_combine(\n",
        "                      lambda predicted, actual: (predicted - actual) ** 2,\n",
        "                      predicted,\n",
        "                      actual)\n",
        "    # And just add them up\n",
        "    return tensor_sum(squared_errors)\n",
        "\n",
        "  def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "    return tensor_combine(\n",
        "        lambda predicted, actual: 2 * (predicted - actual),\n",
        "        predicted,\n",
        "        actual)\n",
        "\n",
        "sse_loss = SSE()\n",
        "assert sse_loss.loss([1, 2, 3], [10, 20, 30]) == 9 ** 2 + 18 ** 2 + 27 ** 2\n",
        "assert sse_loss.gradient([1, 2, 3], [10, 20, 30]) == [-18, -36, -54]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcm3xqmz3QKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need an absract optimizer so we can create different variants of gradient descent\n",
        "\n",
        "class Optimizer:\n",
        "  \"\"\"\n",
        "  An optimizer updates the weights of a layer (in place) using information\n",
        "  known by either the layer or the optimizer (or by both).\n",
        "  \"\"\"\n",
        "  def step(self, layer: Layer) -> None:\n",
        "    raise NotImplementedError\n",
        "  \n",
        "class GradientDescent(Optimizer):\n",
        "  def __init__(self, learning_rate: float = 0.1) -> None:\n",
        "    self.lr = learning_rate\n",
        "  \n",
        "  def step(self, layer: Layer) -> None:\n",
        "    for param, grad in zip(layer.params(), layer.grads()):\n",
        "      # Update param using a gradient step\n",
        "      param[:] = tensor_combine(\n",
        "          lambda param, grad: param - grad * self.lr,\n",
        "          param,\n",
        "          grad)\n",
        "\n",
        "# Note the slice operator. You have to use this if you want to affect the original list. For example\n",
        "tensor = [[1,2],[3,4]]\n",
        "\n",
        "for row in tensor:\n",
        "  row = [0,0]\n",
        "assert tensor == [[1,2],[3,4]], \"assignment doesn't update a list\"\n",
        "\n",
        "for row in tensor:\n",
        "  row[:] = [0,0]\n",
        "assert tensor == [[0,0], [0,0]], \"but slice assignment does\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjm1GougwwY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's show the flexibility of this design by implmenting one more optimizer\n",
        "\n",
        "class Momentum(Optimizer):\n",
        "    def __init__(self,\n",
        "                 learning_rate: float,\n",
        "                 momentum: float = 0.9) -> None:\n",
        "        self.lr = learning_rate\n",
        "        self.mo = momentum\n",
        "        self.updates: List[Tensor] = []  # running average\n",
        "\n",
        "    def step(self, layer: Layer) -> None:\n",
        "        # If we have no previous updates, start with all zeros.\n",
        "        if not self.updates:\n",
        "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
        "\n",
        "        for update, param, grad in zip(self.updates,\n",
        "                                       layer.params(),\n",
        "                                       layer.grads()):\n",
        "            # Apply momentum\n",
        "            update[:] = tensor_combine(\n",
        "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
        "                update,\n",
        "                grad)\n",
        "\n",
        "            # Then take a gradient step\n",
        "            param[:] = tensor_combine(\n",
        "                lambda p, u: p - self.lr * u,\n",
        "                param,\n",
        "                update)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIfK4VQe2Cpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's revisit XOR\n",
        "\n",
        "# training data\n",
        "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
        "ys = [[0.], [1.], [1.], [0.]]\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "net = Sequential([\n",
        "    Linear(input_dim=2, output_dim=2),\n",
        "    Sigmoid(),\n",
        "    Linear(input_dim=2, output_dim=1)\n",
        "])\n",
        "\n",
        "import tqdm\n",
        "\n",
        "optimizer = GradientDescent(learning_rate=0.1)\n",
        "loss = SSE()\n",
        "\n",
        "with tqdm.trange(3000) as t:\n",
        "    for epoch in t:\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for x, y in zip(xs, ys):\n",
        "            predicted = net.forward(x)\n",
        "            epoch_loss += loss.loss(predicted, y)\n",
        "            gradient = loss.gradient(predicted, y)\n",
        "            net.backward(gradient)\n",
        "\n",
        "            optimizer.step(net)\n",
        "\n",
        "        t.set_description(f\"xor loss {epoch_loss:.3f}\")\n",
        "\n",
        "for param in net.params():\n",
        "    print(param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvAHjZQD-cJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One replacement for sigmoid is tanh\n",
        "\n",
        "import math\n",
        "\n",
        "def tanh(x: float) -> float:\n",
        "  # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
        "  # We check for this because, e.g., math.exp(1000) raises an error.\n",
        "  if x < -100: return -1\n",
        "  elif x > 100: return 1\n",
        "\n",
        "  em2x = math.exp(-2 * x)\n",
        "  return (1 - em2x) / (1 + em2x)\n",
        "\n",
        "class Tanh(Layer):\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    # Save tanh output to use in backward pass.\n",
        "    self.tanh = tensor_apply(tanh, input)\n",
        "    return self.tanh\n",
        "\n",
        "  def backward(self, gradient: Tensor) -> Tensor:\n",
        "    return tensor_combine(\n",
        "        lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
        "        self.tanh,\n",
        "        gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDVcd2KfbEht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Another popular one is relu\n",
        "\n",
        "class Relu(Layer):\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    self.input = input\n",
        "    return tensor_apply(lambda x: max(x, 0), input)\n",
        "\n",
        "  def backward(self, gradient: Tensor) -> Tensor:\n",
        "    return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
        "                          self.input,\n",
        "                          gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTx8Ldwjbi9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fizzbuzz revisted\n",
        "\n",
        "from Chapter_18 import binary_encode, fizz_buzz_encode, argmax\n",
        "\n",
        "xs = [binary_encode(n) for n in range(101, 1024)]\n",
        "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
        "\n",
        "NUM_HIDDEN = 25\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "net = Sequential([\n",
        "                  Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
        "                  Tanh(),\n",
        "                  Linear(input_dim=NUM_HIDDEN, output_dim=4, init=\"uniform\"),\n",
        "                  Sigmoid()\n",
        "])\n",
        "\n",
        "def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n",
        "  num_correct = 0\n",
        "  for n in range(low, hi):\n",
        "    x = binary_encode(n)\n",
        "    predicted = argmax(net.forward(x))\n",
        "    actual = argmax(fizz_buzz_encode(n))\n",
        "    if predicted == actual:\n",
        "      num_correct += 1\n",
        "\n",
        "  return num_correct / (hi - low)\n",
        "\n",
        "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
        "loss = SSE()\n",
        "\n",
        "with tqdm.trange(1000) as t:\n",
        "  for epoch in t:\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for x, y in zip(xs, ys):\n",
        "      predicted = net.forward(x)\n",
        "      epoch_loss += loss.loss(predicted, y)\n",
        "      gradient = loss.gradient(predicted, y)\n",
        "      net.backward(gradient)\n",
        "\n",
        "      optimizer.step(net)\n",
        "    \n",
        "    accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
        "    t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n",
        "\n",
        "# Now check results on the test set\n",
        "print(\"test results\", fizzbuzz_accuracy(1, 101, net))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIRaAo01iSBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(tensor: Tensor) -> Tensor:\n",
        "  \"\"\"Softmad along the last dimension\"\"\"\n",
        "  if is_1d(tensor):\n",
        "    # Subtract the largest value for numerical stability.\n",
        "    largest = max(tensor)\n",
        "    exps = [math.exp(x - largest) for x in tensor]\n",
        "\n",
        "    sum_of_exps = sum(exps)  # This is the total \"weight\"\n",
        "    return [exp_i / sum_of_exps # Probability is the fraction\n",
        "            for exp_i in exps] # of the total weight\n",
        "  else:\n",
        "    return [softmax(tensor_i) for tensor_i in tensor]\n",
        "\n",
        "class SoftmaxCrossEntropy(Loss):\n",
        "  \"\"\"\n",
        "  This is the negative-log-likelihood of the ovserved values, given the\n",
        "  neural net model. So if we choose weights ot minimize it, out model will\n",
        "  be maximinzing the likelihood of the observed data.\n",
        "  \"\"\"\n",
        "  def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = softmax(predicted)\n",
        "\n",
        "    # This will be log p_i for the actual class i and 0 for the other\n",
        "    # classes. We add a tiny amount to p to avoid taking log(0).\n",
        "    likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
        "                                 probabilities,\n",
        "                                 actual)\n",
        "    \n",
        "    # And then we just sum up the negatives.\n",
        "    return -tensor_sum(likelihoods)\n",
        "\n",
        "  def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "    probabilities = softmax(predicted)\n",
        "\n",
        "    # Isn't this a pleasant equation:\n",
        "    return tensor_combine(lambda p, actual: p - actual,\n",
        "                          probabilities,\n",
        "                          actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJln8vzcj4bC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(0)\n",
        "from Chapter_18 import binary_encode, fizz_buzz_encode, argmax\n",
        "\n",
        "xs = [binary_encode(n) for n in range(101, 1024)]\n",
        "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n",
        "NUM_HIDDEN = 25\n",
        "net = Sequential([\n",
        "                  Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
        "                  Tanh(),\n",
        "                  Linear(input_dim=NUM_HIDDEN, output_dim=4, init=\"uniform\"),\n",
        "                  # No final Sigmoid layer now\n",
        "])\n",
        "\n",
        "def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n",
        "  num_correct = 0\n",
        "  for n in range(low, hi):\n",
        "    x = binary_encode(n)\n",
        "    predicted = argmax(net.forward(x))\n",
        "    actual = argmax(fizz_buzz_encode(n))\n",
        "    if predicted == actual:\n",
        "      num_correct += 1\n",
        "\n",
        "  return num_correct / (hi - low)\n",
        "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
        "loss = SoftmaxCrossEntropy()\n",
        "\n",
        "with tqdm.trange(1000) as t:\n",
        "  for epoch in t:\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for x, y in zip(xs, ys):\n",
        "      predicted = net.forward(x)\n",
        "      epoch_loss += loss.loss(predicted, y)\n",
        "      gradient = loss.gradient(predicted, y)\n",
        "      net.backward(gradient)\n",
        "\n",
        "      optimizer.step(net)\n",
        "    \n",
        "    accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
        "    t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n",
        "\n",
        "# Now check results on the test set\n",
        "print(\"test results\", fizzbuzz_accuracy(1, 101, net))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NduyXxAnKmG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}