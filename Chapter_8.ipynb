{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_8.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8evGAPas7swtNgaodfmaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaoJTtionPqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "# !git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "#%cd data-science-from-scratch-Python/\n",
        "#!pip install import-ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV-cKWQ-ZBBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient descent\n",
        "import import_ipynb\n",
        "from Chapter_4 import Vector, dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABrb-hu_ZiVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A lot of times, we'll want to minimize the following type of function\n",
        "def sum_of_squares(v: Vector) -> float:\n",
        "  \"\"\"Conputes the sum of squared elements in v\"\"\"\n",
        "  return dot(v,v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOpctPlUZwSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Callable\n",
        "\n",
        "# Derivative is defined as the limit of the difference quotients of a function f\n",
        "def difference_quotient(f: Callable[[float], float],\n",
        "                        x: float,\n",
        "                        h: float) -> float:\n",
        "    return (f(x + h) - f(x)) / h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9izc_albVOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For many functions, you can calculate the derivative explicitly\n",
        "def square(x:float) -> float:\n",
        "  return x * x\n",
        "\n",
        "def derivative(x: float) -> float:\n",
        "  return 2 * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LctPedPrcqIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# But a lot of the times,  you'll have to estimate derivatives\n",
        "# Let's see how they compare\n",
        "\n",
        "xs = range(-10, 11)\n",
        "actuals = [derivative(x) for x in xs]\n",
        "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
        "\n",
        "# plot to show they're basically the same\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title(\"Actual Derivatives vs. Estimates\")\n",
        "plt.plot(xs, actuals, 'rx', label='Actual')\n",
        "plt.plot(xs, estimates, 'b+', label='Estimate')\n",
        "plt.legend(loc=9)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmxgGF11dhml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Partial derivatives are necessary a lot of the times\n",
        "def partial_difference_quotient(f: Callable[[Vector], float],\n",
        "                                v: Vector,\n",
        "                                i: int,\n",
        "                                h: float) -> float:\n",
        "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
        "    w = [v_j + (h if j == i else 0)     # add h to just the ith element of v\n",
        "         for j, v_j in enumerate(v)]\n",
        "    \n",
        "    return (f(w) - f(v)) / h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d7xDhuE8P4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def estimate_gradient(f: Callable[[Vector], float],\n",
        "                      v: Vector,\n",
        "                      h: float = 0.0001):\n",
        "  return [partial_difference_quotient(f, v, i, h)\n",
        "          for i in range(len(v))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxyemBW86-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One drawback of using difference quotients is that it's computationally expensive\n",
        "\n",
        "# Usually, you just pick a random starting point and take tiny steps in the opposite direction of the gradient until the gradient is very small\n",
        "import random\n",
        "from Chapter_4 import distance, add, scalar_multiply\n",
        "\n",
        "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
        "  \"\"\"Moves 'step_size' in the 'gradient' directions from 'v'\"\"\"\n",
        "  assert len(v) == len(gradient)\n",
        "  step = scalar_multiply(step_size, gradient)\n",
        "  return add(v, step)\n",
        "\n",
        "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
        "  return [2 * v_i for v_i in v]\n",
        "\n",
        "# pick a random starting point\n",
        "v = [random.uniform(-10, 10) for i in range(3)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "  grad = sum_of_squares_gradient(v)     # compute the gradient at v\n",
        "  v = gradient_step(v, grad, -0.01)     # take a negative gradient step\n",
        "  print(epoch, v)\n",
        "\n",
        "assert distance(v, [0, 0, 0]) < 0.001   # v should be close to 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHF2omaA3cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To fit models, we usually compute a loss function to see how close we are to matching the original data\n",
        "\n",
        "# x ranges from -50 to 49, y is always 20 * x + 5\n",
        "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
        "\n",
        "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
        "  slope, intercept = theta\n",
        "  predicted = slope * x + intercept     # The prediction of the model\n",
        "  error = (predicted - y)               # error is (predicted - actual)\n",
        "  squared_error = error ** 2            # We'll minimize squared error\n",
        "  grad = [2 * error * x, 2 * error]\n",
        "  return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFZpW2xMD33I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Chapter_4 import vector_mean\n",
        "\n",
        "# Start with random values for slope and intercept\n",
        "theta = [random.uniform(-1, 1), random.uniform(-1,1)]\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epoch in range(5000):\n",
        "  # Compute the mean of the gradients\n",
        "  grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
        "  # Take a step in that direction\n",
        "  theta = gradient_step(theta, grad, -learning_rate)\n",
        "  print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBsbOJIQGY-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Minibatch gradient descent for large data sets\n",
        "from typing import TypeVar, List, Iterator\n",
        "\n",
        "T = TypeVar('T') # this allows us to type \"generic\" functions\n",
        "\n",
        "def minibatches(dataset: List[T],\n",
        "                batch_size: int,\n",
        "                shuffle: bool = True) -> Iterator[List[T]]:\n",
        "  \"\"\"Generates 'batch_size' - sized minibatches from the dataset\"\"\"\n",
        "  # start indexes 0, batch_size, 2 * batch_size, ...\n",
        "  batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
        "\n",
        "  if shuffle: random.shuffle(batch_starts) # shuffle the batches\n",
        "\n",
        "  for start in batch_starts:\n",
        "    end = start + batch_size\n",
        "    yield dataset[start:end]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7MjvdcxJ3Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can solve our problem using minibatches\n",
        "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "  for batch in minibatches(inputs, batch_size=20):\n",
        "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
        "    theta = gradient_step(theta, grad, -learning_rate)\n",
        "  print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roXWWBz3KpVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Another variation is stochastic gradient descent, in which you take gradient steps on example at a time\n",
        "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "\n",
        "for epoch in range(100):\n",
        "  for x, y in inputs:\n",
        "    grad = linear_gradient(x, y, theta)\n",
        "    theta = gradient_step(theta, grad, -learning_rate)\n",
        "  print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}