{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYRe9mpgGBPfDs6dGoFD6L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsxLm6qyXAAr"
      },
      "source": [
        "# Scraping the web. We'll use a couple external packages (i.e., not from scratch)\n",
        "#!pip install beautifulsoup4 requests html5lib\n",
        "\n",
        "# Only do this once per VM, otherwise you'll get multiple clones and nested directories\n",
        "#!git clone https://github.com/jamestheengineer/data-science-from-scratch-Python.git\n",
        "#%cd data-science-from-scratch-Python/\n",
        "#!pip install import-ipynb\n",
        "#!pip install pillow\n",
        "#import import_ipynb\n",
        "\n",
        "# NLP\n",
        "\n",
        "data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
        "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
        "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
        "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
        "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
        "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
        "         (\"thought leadership\", 35, 35)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX6w3TjxXSid"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def text_size(total: int) -> float:\n",
        "  \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\"\n",
        "  return 8 + total / 200 * 20\n",
        "\n",
        "for word, job_popularity, resume_popularity in data:\n",
        "  plt.text(job_popularity, resume_popularity, word, \n",
        "           ha='center', va='center',\n",
        "           size=text_size(job_popularity + resume_popularity))\n",
        "\n",
        "plt.xlabel(\"Popularity on Job Postings\")\n",
        "plt.ylabel(\"Popularity on Resumes\")\n",
        "plt.axis([0,100,0,100])\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M1efyogYLDB"
      },
      "source": [
        "# Make a bunch of webpages example\n",
        "def fix_unicode(text: str) -> str:\n",
        "    return text.replace(u\"\\u2019\", \"'\")\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://www.oreilly.com/ideas/what-is-data-science\"\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "content = soup.find(\"div\", \"main-post-radar-content\")   # find right content div\n",
        "regex = r\"[\\w']+|[\\.]\"                       # matches a word or a period\n",
        "document = []\n",
        "\n",
        "for paragraph in content(\"p\"):\n",
        "    words = re.findall(regex, fix_unicode(paragraph.text))\n",
        "    document.extend(words)\n",
        "\n",
        "print(document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd6wfyKh6rwF"
      },
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "transitions = defaultdict(list)\n",
        "for prev, current in zip(document, document[1:]):\n",
        "  transitions[prev].append(current)\n",
        "\n",
        "def generate_using_bigrams() -> str:\n",
        "  current = \".\" # this means the next word will start a sentence\n",
        "  result = []\n",
        "  while True:\n",
        "    next_word_candidates = transitions[current] # bigrams (current, _)\n",
        "    current = random.choice(next_word_candidates) # choose one at random\n",
        "    result.append(current) # append it to results\n",
        "    if current == \".\": return \" \".join(result) # if \".\" we're done\n",
        "\n",
        "# You can make sentences less gibberishly by using trigrams instead of bigrams\n",
        "trigram_transitions = defaultdict(list)\n",
        "starts = []\n",
        "for prev, current, next in zip(document, document[1:], document[2:]):\n",
        "  if prev == \".\":  # if the previous word was a period\n",
        "    starts.append(current) # then this is a start word\n",
        "  \n",
        "  trigram_transitions[(prev, current)].append(next)\n",
        "\n",
        "def generate_using_trigrams() -> str:\n",
        "  current = random.choice(starts)\n",
        "  prev = \".\"\n",
        "  result = [current]\n",
        "  while True:\n",
        "    next_word_candidates = trigram_transitions[(prev, current)]\n",
        "    next_word = random.choice(next_word_candidates)\n",
        "\n",
        "    prev, current = current, next_word\n",
        "    result.append(current)\n",
        "\n",
        "    if current == \".\":\n",
        "      return \" \".join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qm-XrJ5ilAx"
      },
      "source": [
        "# Grammars\n",
        "from typing import List, Dict\n",
        "\n",
        "# Type alias to refer to grammars later\n",
        "Grammar = Dict[str, List[str]]\n",
        "\n",
        "grammar = {\n",
        "    \"_S\"  : [\"_NP _VP\"],\n",
        "    \"_NP\" : [\"_N\",\n",
        "             \"_A _NP _P _A _N\"],\n",
        "    \"_VP\" : [\"_V\",\n",
        "             \"_V _NP\"],\n",
        "    \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
        "    \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
        "    \"_P\"  : [\"about\", \"near\"],\n",
        "    \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
        "}\n",
        "\n",
        "def is_terminal(token: str) -> bool:\n",
        "  return token[0] != \"_\"\n",
        "\n",
        "def expand(grammar: Grammar, tokens: List[str]) -> List[str]:\n",
        "  for i, token in enumerate(tokens):\n",
        "    # if this is a terminal token, skip it\n",
        "    if is_terminal(token): continue\n",
        "\n",
        "    # Otherwise, it's a nonterminal token, \n",
        "    # so we need to choose a replacement at random.\n",
        "    replacement = random.choice(grammar[token])\n",
        "\n",
        "    if is_terminal(replacement):\n",
        "      tokens[i] = replacement\n",
        "    else:\n",
        "      # Replacement be another nonterminal\n",
        "      tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
        "    \n",
        "    # now call expand on the new list of tokens\n",
        "    return expand(grammar, tokens)\n",
        "  # If we get here, we had all terminals and are done\n",
        "  return tokens\n",
        "\n",
        "def generate_sentence(grammar: Grammar) -> List[str]:\n",
        "  return expand(grammar, [\"_S\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS1gHd54rvUF"
      },
      "source": [
        "print(generate_sentence(grammar))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsEtpApjuGP3"
      },
      "source": [
        "# Gibbs sampling - a way to sample hard to sample distributions\n",
        "\n",
        "# Rolling a dice example\n",
        "from typing import Tuple\n",
        "import random\n",
        "\n",
        "def roll_a_die() -> int:\n",
        "  return random.choice([1,2,3,4,5,6])\n",
        "\n",
        "def direct_sample() -> Tuple[int, int]:\n",
        "  d1 = roll_a_die()\n",
        "  d2 = roll_a_die()\n",
        "  return d1, d1 + d2\n",
        "\n",
        "def random_y_given_x(x: int) -> int:\n",
        "  \"\"\"equally likely to be x+1, x+2, ..., x+6\"\"\"\n",
        "  return x + roll_a_die()\n",
        "\n",
        "def random_x_given_y(y: int) -> int:\n",
        "  if y <= 7:\n",
        "    # if the total is 7 or less, the first die is equally likely to be\n",
        "    # 1, 2, ...., (total - 1)\n",
        "    return random.randrange(1, y)\n",
        "  else:\n",
        "    # if the total is 7 or more, the first die is equally likely to be\n",
        "    # (total - 6), (total - 5), ..., 6\n",
        "    return random.randrange(y - 6, 7)\n",
        "\n",
        "def gibbs_sample(num_iters: int = 100) -> Tuple[int, int]:\n",
        "  x, y = 1, 2 # doesn't really matter\n",
        "  for _ in range(num_iters):\n",
        "    x = random_x_given_y(y)\n",
        "    y = random_y_given_x(x)\n",
        "  return x, y\n",
        "\n",
        "def compare_distributions(num_samples: int = 1000) -> Dict[int, List[int]]:\n",
        "  counts = defaultdict(lambda: [0,0])\n",
        "  for _ in range(num_samples):\n",
        "    counts[gibbs_sample()][0] += 1\n",
        "    counts[direct_sample()][1] += 1\n",
        "  return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pXbazEpAFDc"
      },
      "source": [
        "def sample_from(weights: List[float]) -> int:\n",
        "  \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
        "  total = sum(weights)\n",
        "  rnd = total * random.random() # uniform between 0 and total\n",
        "  for i, w in enumerate(weights):\n",
        "    rnd -= w      # return the smallest i such that\n",
        "    if rnd <= 0: return i # weights[0] + ... + weights[i] >= rnd\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Draw 1000 times and count\n",
        "draws = Counter(sample_from([0.1, 0.1, 0.8]) for _ in range(1000))\n",
        "assert 10 < draws[0] < 190 # should be ~10%, this is really a loose test\n",
        "assert 10 < draws[1] < 190 # should be ~10%, this is really a loose test\n",
        "assert 650 < draws[2] < 950 # should be ~80%, this is a really loose test\n",
        "assert draws[0] + draws[1] + draws[2] == 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaSU_ZZdPA7K"
      },
      "source": [
        "documents = [\n",
        "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
        "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
        "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
        "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
        "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
        "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
        "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
        "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
        "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
        "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
        "    [\"statistics\", \"R\", \"statsmodels\"],\n",
        "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
        "    [\"pandas\", \"R\", \"Python\"],\n",
        "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
        "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpVzso-EPQHX"
      },
      "source": [
        "K = 4\n",
        "\n",
        "# a list of Counters, one for each document\n",
        "document_topic_counts = [Counter() for _ in documents]\n",
        "\n",
        "# a list of Counters, one for each topic\n",
        "topic_word_counts = [Counter() for _ in range(K)]\n",
        "\n",
        "# a list of numbers, one for each topic\n",
        "topic_counts = [0 for _ in range(K)]\n",
        "\n",
        "# a list of numbers, one for each document\n",
        "document_lengths = [len(document) for document in documents]\n",
        "\n",
        "# number of distinct words\n",
        "distinct_words = set(word for document in documents for word in document)\n",
        "W = len(distinct_words)\n",
        "\n",
        "# number of documents\n",
        "D = len(documents)\n",
        "\n",
        "print(D)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9ZapyLI3Zgk"
      },
      "source": [
        "def p_topic_given_document(topic: int, d: int, alpha: float = 0.1) -> float:\n",
        "  \"\"\"\n",
        "  The fraction of words in document 'd'\n",
        "  that are assigned to 'topic' (plus some smoothing)\n",
        "  \"\"\"\n",
        "  return ((document_topic_counts[d][topic] + alpha) / \n",
        "          (document_lengths[d] + K * alpha))\n",
        "\n",
        "def p_word_given_topic(word: str, topic: int, beta: float = 0.1) -> float:\n",
        "  \"\"\"\n",
        "  The fraction of words assigned to 'topic'\n",
        "  that equal 'word' (plus some smoothing)\n",
        "  \"\"\"\n",
        "  return ((topic_word_counts[topic][word] + beta) /\n",
        "          (topic_counts[topic] + W * beta))\n",
        "  \n",
        "def topic_weight(d: int, word: str, k: int) -> float:\n",
        "  \"\"\"\n",
        "  Given a document and a word in that document,\n",
        "  return the weight for the kth topic\n",
        "  \"\"\"\n",
        "  return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
        "\n",
        "def choose_new_topic(d: int, word: str) -> int:\n",
        "  return sample_from([topic_weight(d, word, k)\n",
        "                      for k in range(K)])\n",
        "  \n",
        "random.seed(0)\n",
        "document_topics = [[random.randrange(K) for word in document]\n",
        "                   for document in documents]\n",
        "\n",
        "for d in range(D):\n",
        "  for word, topic in zip(documents[d], document_topics[d]):\n",
        "    document_topic_counts[d][topic] += 1\n",
        "    topic_word_counts[topic][word] += 1\n",
        "    topic_counts[topic] += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4o-7cqj5x5c"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "for iter in tqdm.trange(1000):\n",
        "  for d in range(D):\n",
        "    for i, (word, topic) in enumerate(zip(documents[d],\n",
        "                                          document_topics[d])):\n",
        "      # remove this word / topic from the counts\n",
        "      # so that it doesn't influence the weights\n",
        "      document_topic_counts[d][topic] -= 1\n",
        "      topic_word_counts[topic][word] -= 1\n",
        "      topic_counts[topic] -= 1\n",
        "      document_lengths[d] -= 1\n",
        "\n",
        "      # choose a new topic based on the weights\n",
        "      new_topic = choose_new_topic(d, word)\n",
        "      document_topics[d][i] = new_topic\n",
        "\n",
        "      # and now add it back to the counts\n",
        "      document_topic_counts[d][new_topic] += 1\n",
        "      topic_word_counts[new_topic][word] += 1\n",
        "      topic_counts[new_topic] += 1\n",
        "      document_lengths[d] += 1\n",
        "\n",
        "for k, word_counts in enumerate(topic_word_counts):\n",
        "  for word, count in word_counts.most_common():\n",
        "    if count > 0:\n",
        "      print(k, word, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCnjcCwm4-AB"
      },
      "source": [
        "# The above results are stochastic, so the below topics might not be true\n",
        "topic_names = [\"Big data and programming languages\",\n",
        "               \"Python and statistics\",\n",
        "               \"databases\",\n",
        "               \"artificial intelligence\"]\n",
        "\n",
        "for document, topic_counts in zip(documents, document_topic_counts):\n",
        "  print(document)\n",
        "  for topic, count in topic_counts.most_common():\n",
        "    if count > 0:\n",
        "      print(topic_names[topic], count)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBaAKG2w84vn"
      },
      "source": [
        "# Deep learning is the new hotness\n",
        "from Chapter_04 import dot, Vector\n",
        "import math\n",
        "\n",
        "def cosine_similarity(v1: Vector, v2: Vector) -> float:\n",
        "  return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2))\n",
        "\n",
        "assert cosine_similarity([1., 1, 1],[2.,2,2]) == 1 # same direction\n",
        "assert cosine_similarity([-1.,-1],[2.,2]) == -1 # opposite direction\n",
        "assert cosine_similarity([1.,0],[0.,1]) == 0 # orthogonal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-JkpU9uD1vc"
      },
      "source": [
        "colors = [\"red\", \"green\", \"blue\",\"yellow\",\"black\",\"\"]\n",
        "nouns = [\"bed\",\"car\",\"boat\",\"cat\"]\n",
        "verbs = [\"is\", \"was\",\"seems\"]\n",
        "adverbs=[\"very\",\"quite\",\"extremely\",\"\"]\n",
        "adjectives=[\"slow\",\"fast\",\"soft\",\"hard\"]\n",
        "\n",
        "def make_sentence() -> str:\n",
        "  return \" \".join([\n",
        "      \"The\",\n",
        "      random.choice(colors),\n",
        "      random.choice(nouns),\n",
        "      random.choice(verbs),\n",
        "      random.choice(adverbs),\n",
        "      random.choice(adjectives),\n",
        "      \".\"\n",
        "  ])\n",
        "\n",
        "NUM_SENTENCES = 50\n",
        "random.seed(0)\n",
        "sentences = [make_sentence() for _ in range(NUM_SENTENCES)]\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxEaOBzsyWiw"
      },
      "source": [
        "# Let's make a vocabulary class to start some real work\n",
        "from Chapter_19 import Tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsHJGfwdymru"
      },
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self, words: List[str] = None) -> None:\n",
        "    self.w2i: Dict[str, int] = {} # mapping word -> word_id\n",
        "    self.i2w: Dict[int, str] = {} # mapping word_id -> word\n",
        "\n",
        "    for word in (words or []):  # if words were provided\n",
        "      self.add(word)            # add them\n",
        "\n",
        "  @property\n",
        "  def size(self) -> int:\n",
        "    \"\"\"how many words are in the vocabulary\"\"\"\n",
        "    return len(self.w2i)\n",
        "  \n",
        "  def add(self, word: str) -> None:\n",
        "    if word not in self.w2i:\n",
        "      word_id = len(self.w2i)\n",
        "      self.w2i[word] = word_id\n",
        "      self.i2w[word_id] = word\n",
        "    \n",
        "  def get_id(self, word: str) -> int:\n",
        "    \"\"\"return the id of the word or None\"\"\"\n",
        "    return self.w2i.get(word)\n",
        "\n",
        "  def get_word(self, word_id: int) -> str:\n",
        "    \"\"\"return the word with the given id or None\"\"\"\n",
        "    return self.i2w.get(word_id)\n",
        "\n",
        "  def one_hot_encode(self, word: str) -> Tensor:\n",
        "    word_id = self.get_id(word)\n",
        "    assert word_id is not None, f\"unknown word {word}\"\n",
        "\n",
        "    return [1.0 if i == word_id else 0.0 for i in range(self.size)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD8OhswX6rJm"
      },
      "source": [
        "vocab = Vocabulary([\"a\",\"b\",\"c\"])\n",
        "assert vocab.size == 3\n",
        "assert vocab.get_id(\"b\") == 1\n",
        "assert vocab.one_hot_encode(\"b\") == [0,1,0]\n",
        "assert vocab.get_id(\"z\") is None\n",
        "assert vocab.get_word(2) == \"c\"\n",
        "vocab.add(\"z\")\n",
        "assert vocab.size == 4\n",
        "assert vocab.get_id(\"z\") == 3\n",
        "assert vocab.one_hot_encode(\"z\") == [0, 0, 0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT8iUjdI7mr1"
      },
      "source": [
        "import json\n",
        "\n",
        "def save_vocab(vocab: Vocabulary, filename: str) -> None:\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(vocab.w2i, f) # Only need to save w2i\n",
        "\n",
        "def load_vocab(filename: str) -> Vocabulary:\n",
        "  vocab = Vocabulary()\n",
        "  with open(filename) as f:\n",
        "    # Load w2i and generate i2w from it\n",
        "    vocab.w2i = json.load(f)\n",
        "    vocab.i2w = {id: word for word, id in vocab.w2i.items()}\n",
        "  return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbTeTr6duAZP"
      },
      "source": [
        "from typing import Iterable\n",
        "from Chapter_19 import Layer, Tensor, random_tensor, zeros_like\n",
        "\n",
        "class Embedding(Layer):\n",
        "  def __init__(self, num_embeddings: int, embedding_dim: int) -> None:\n",
        "    self.num_embeddings = num_embeddings\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # One vector of size embedding_dim for each desired embedding\n",
        "    self.embeddings = random_tensor(num_embeddings, embedding_dim)\n",
        "    self.grad = zeros_like(self.embeddings)\n",
        "\n",
        "    # Save last input id\n",
        "    self.last_input_id = None\n",
        "\n",
        "  def forward(self, input_id: int) -> Tensor:\n",
        "    \"\"\"Just select the embedding vector corresponding to the input id\"\"\"\n",
        "    self.input_id = input_id # remember for use in backpropagation\n",
        "\n",
        "    return self.embeddings[input_id]\n",
        "\n",
        "  def backward(self, gradient: Tensor) -> None:\n",
        "    # Zero out the gradient corresponding to the last input\n",
        "    # This is way cheaper than creating a new all-zero tensor each time\n",
        "    if self.last_input_id is not None:\n",
        "      zero_row = [0 for _ in range(self.embedding_dim)]\n",
        "      self.grad[self.last_input_id] = zero_row\n",
        "    \n",
        "    self.last_input_id = self.input_id\n",
        "    self.grad[self.input_id] = gradient\n",
        "\n",
        "  def params(self) -> Iterable[Tensor]:\n",
        "    return [self.embeddings]\n",
        "\n",
        "  def grads(self) -> Iterable[Tensor]:\n",
        "    return [self.grad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxn2Mv1c9jiu"
      },
      "source": [
        "class TextEmbedding(Embedding):\n",
        "  def __init__(self, vocab: Vocabulary, embedding_dim: int) -> None:\n",
        "    # Call the superclass constructor\n",
        "    super().__init__(vocab.size, embedding_dim)\n",
        "\n",
        "    # And hang onto the vocab\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def __getitem__(self, word: str) -> Tensor:\n",
        "    word_id = self.vocab.get_id(word)\n",
        "    if word_id is not None:\n",
        "      return self.embeddings[word_id]\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def closest(self, word: str, n: int = 5) -> List[Tuple[float, str]]:\n",
        "    \"\"\"Returns the n closest words based on cosine similarity\"\"\"\n",
        "    vector = self[word]\n",
        "\n",
        "    # Compute pairs (similarity, other_word), and sort most similar first\n",
        "    scores = [(cosine_similarity(vector, self.embeddings[i]), other_word)\n",
        "              for other_word, i in self.vocab.w2i.items()]\n",
        "    scores.sort(reverse=True)\n",
        "\n",
        "    return scores[:n]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt6sDecl_btM"
      },
      "source": [
        "import re\n",
        "\n",
        "# This is not a great regex, but it works on our data\n",
        "tokenized_sentences = [re.findall(\"[a-z]+|[.]\", sentence.lower())\n",
        "                        for sentence in sentences]\n",
        "\n",
        "# Create a vocabulary (that is, a mapping word -> word_id) based on our text.\n",
        "vocab = Vocabulary(word\n",
        "                   for sentence_words in tokenized_sentences\n",
        "                   for word in sentence_words)\n",
        "\n",
        "from Chapter_19 import Tensor\n",
        "\n",
        "inputs: List[int] = []\n",
        "targets: List[Tensor] = []\n",
        "\n",
        "for sentence in tokenized_sentences:\n",
        "  for i, word in enumerate(sentence):\n",
        "    for j in [i - 2, i - 1, i + 1, i + 2]:\n",
        "      if 0 <= j < len(sentence):\n",
        "        nearby_word = sentence[j]\n",
        "\n",
        "        # Add an input that's the original word_id\n",
        "        inputs.append(vocab.get_id(word))\n",
        "\n",
        "        # Add a target that's the one-hot-encoded nearby word\n",
        "        targets.append(vocab.one_hot_encode(nearby_word))\n",
        "\n",
        "from Chapter_19 import Sequential, Linear\n",
        "\n",
        "random.seed(0)\n",
        "EMBEDDING_DIM = 5\n",
        "\n",
        "# Define the embedding layer separately, so we can reference it\n",
        "embedding = TextEmbedding(vocab=vocab, embedding_dim=EMBEDDING_DIM)\n",
        "\n",
        "model = Sequential([\n",
        "                    # Given a word as a vector of word_ids, look up its embedding\n",
        "                    embedding,\n",
        "                    # And use a linear layer to compute scores for nearby words\n",
        "                    Linear(input_dim=EMBEDDING_DIM, output_dim=vocab.size)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSEsa4UDBhiq"
      },
      "source": [
        "# Let's get ready to train\n",
        "from Chapter_19 import SoftmaxCrossEntropy, Momentum, GradientDescent\n",
        "\n",
        "loss = SoftmaxCrossEntropy()\n",
        "optimizer = GradientDescent(learning_rate = 0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "  epoch_loss = 0.0\n",
        "  for input, target in zip(inputs, targets):\n",
        "    predicted = model.forward(input)\n",
        "    epoch_loss += loss.loss(predicted, target)\n",
        "    gradient = loss.gradient(predicted, target)\n",
        "    model.backward(gradient)\n",
        "    optimizer.step(model)\n",
        "  print(epoch, epoch_loss)\n",
        "  print(embedding.closest(\"black\"))\n",
        "  print(embedding.closest(\"slow\"))\n",
        "  print(embedding.closest(\"car\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8djgkztWkw-"
      },
      "source": [
        "pairs = [(cosine_similarity(embedding[w1], embedding[w2]), w1, w2)\n",
        "          for w1 in vocab.w2i\n",
        "          for w2 in vocab.w2i\n",
        "          if w1 < w2]\n",
        "pairs.sort(reverse=True)\n",
        "print(pairs[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxh07a1hUzhG"
      },
      "source": [
        "from Chapter_10 import pca, transform\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the first two principal components and transform the word vectors\n",
        "components = pca(embedding.embeddings, 2)\n",
        "transformed = transform(embedding.embeddings, components)\n",
        "\n",
        "# Scatter the points (and make them white so they're \"invisible\")\n",
        "flg, ax = plt.subplots()\n",
        "ax.scatter(*zip(*transformed), marker='.', color='w')\n",
        "\n",
        "# Add annotations for each word at its transformed location\n",
        "for word, idx in vocab.w2i.items():\n",
        "  ax.annotate(word, transformed[idx])\n",
        "\n",
        "# And hide the axes\n",
        "ax.get_xaxis().set_visible(False)\n",
        "ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UHfMszVVF19"
      },
      "source": [
        "# RNNs. Save a hidden state to better handle actual sentences\n",
        "\n",
        "from Chapter_19 import tensor_apply, tanh\n",
        "\n",
        "class SimpleRnn(Layer):\n",
        "  \"\"\"Just about the simplest possible recurrent layer.\"\"\"\n",
        "  def __init__(self, input_dim: int, hidden_dim: int) -> None:\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.w = random_tensor(hidden_dim, input_dim, init = 'xavier')\n",
        "    self.u = random_tensor(hidden_dim, hidden_dim, init = 'xavier')\n",
        "    self.b = random_tensor(hidden_dim)\n",
        "\n",
        "    self.reset_hidden_state()\n",
        "\n",
        "  def reset_hidden_state(self) -> None:\n",
        "    self.hidden = [0 for _ in range(self.hidden_dim)]\n",
        "\n",
        "  def forward(self, input: Tensor) -> Tensor:\n",
        "    self.input = input\n",
        "    self.prev_hidden = self.hidden\n",
        "\n",
        "    a = [(dot(self.w[h], input) +\n",
        "          dot(self.u[h], self.hidden) +\n",
        "          self.b[h])\n",
        "          for h in range(self.hidden_dim)]\n",
        "    \n",
        "    self.hidden = tensor_apply(tanh, a)\n",
        "    return self.hidden\n",
        "  \n",
        "  def backward(self, gradient: Tensor):\n",
        "    # Backpropagation through the tanh\n",
        "    a_grad = [gradient[h] * (1 - self.hidden[h] ** 2)\n",
        "              for h in range(self.hidden_dim)]\n",
        "\n",
        "    # b has the same gradient as a\n",
        "    self.b_grad = a_grad\n",
        "\n",
        "    # Each w[h][i] is multiplied by input[i] and added to a[h],\n",
        "    # so each w_grad[h][i] = a_grad[h] * input[i]\n",
        "    self.w_grad = [[a_grad[h] * self.input[i]\n",
        "                    for i in range(self.input_dim)]\n",
        "                    for h in range(self.hidden_dim)]\n",
        "    \n",
        "    # Each u[h][h2] is multiplied by hidden[h2] and added to a[h],\n",
        "    # so each u_grad[h][h2] = a_grad[h] * prev_hidden[h2]\n",
        "    self.u_grad = [[a_grad[h] * self.prev_hidden[h2]\n",
        "                    for h2 in range(self.hidden_dim)]\n",
        "                   for h in range(self.hidden_dim)]\n",
        "\n",
        "    # Each input[i] is multiplied by every w[h][i] and added to a[h],\n",
        "    # so each input_grad[i] = sum(a_grad[h] * w[h][i] for h in ...)\n",
        "    return [sum(a_grad[h] * self.w[h][i] for h in range(self.hidden_dim))\n",
        "            for i in range(self.input_dim)]\n",
        "\n",
        "    def params(self) -> Iterable[Tensor]:\n",
        "      return [self.w, self.u, self.b]\n",
        "    \n",
        "    def grads(self) -> Iterable[Tensor]:\n",
        "      return [self.w_grad, self.u_grad, self.b_grad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT3uy2dwQ1jb"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://www.ycombinator.com/topcompanies/\"\n",
        "soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
        "\n",
        "# We get the companies twice, so use a set comprehension to dediplicate.\n",
        "companies = list({b.text\n",
        "                  for b in soup(\"b\")\n",
        "                  if \"h4\" in b.get(\"class\",())})\n",
        "print(len(companies))\n",
        "assert len(companies) == 102\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEW-Y2z0-zYy"
      },
      "source": [
        "vocab = Vocabulary([c for company in companies for c in company])\n",
        "\n",
        "START = \"^\"\n",
        "STOP = \"$\"\n",
        "\n",
        "# We need to add them to the vocabulary too.\n",
        "vocab.add(START)\n",
        "vocab.add(STOP)\n",
        "\n",
        "HIDDEN_DIM = 32\n",
        "rnn1 = SimpleRnn(input_dim=vocab.size, hidden_dim=HIDDEN_DIM)\n",
        "rnn2 = SimpleRnn(input_dim=HIDDEN_DIM, hidden_dim=HIDDEN_DIM)\n",
        "linear = Linear(input_dim=HIDDEN_DIM, output_dim=vocab.size)\n",
        "\n",
        "model = Sequential([rnn1, rnn2, linear])\n",
        "\n",
        "from Chapter_19 import softmax\n",
        "\n",
        "def generate(seed: str = START, max_len: int = 50) -> str:\n",
        "  rnn1.reset_hidden_state()\n",
        "  rnn2.reset_hidden_state()\n",
        "  output = [seed]\n",
        "\n",
        "  # Keep going until we produce the stop character or reach max length\n",
        "  while output[-1] != STOP and len(output) < max_len:\n",
        "    # Use the last character as the input\n",
        "    input = vocab.one_hot_encode(output[-1])\n",
        "\n",
        "    # Generate scores using the model\n",
        "    predicted = model.forward(input)\n",
        "\n",
        "    # Conver them to probabilities and draw a randome char_id\n",
        "    probabilities = softmax(predicted)\n",
        "    next_char_id = sample_from(probabilities)\n",
        "\n",
        "    # Add the corresponding char to our output\n",
        "    output.append(vocab.get_word(next_char_id))\n",
        "\n",
        "  # Get rid if START and END characters and return the word\n",
        "  return ''.join(output[1:-1])\n",
        "\n",
        "loss = SoftmaxCrossEntropy()\n",
        "optimizer = Momentum(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(300):\n",
        "  random.shuffle(companies) # Train in a different order each epoch\n",
        "  epoch_loss = 0\n",
        "  for company in tqdm.tqdm(companies):\n",
        "    rnn1.reset_hidden_state()\n",
        "    rnn2.reset_hidden_state()\n",
        "    company = START + company + STOP\n",
        "\n",
        "    # Usual training loop\n",
        "    for prev, next in zip(company, company[1:]):\n",
        "      input = vocab.one_hot_encode(prev)\n",
        "      target = vocab.one_hot_encode(next)\n",
        "      predicted = model.forward(input)\n",
        "      epoch_loss += loss.loss(predicted, target)\n",
        "      gradient = loss.gradient(predicted, target)\n",
        "      model.backward(gradient)\n",
        "      optimizer.step(model)\n",
        "    \n",
        "  print(epoch, epoch_loss, generate())\n",
        "\n",
        "  # Turn down the learning rate for the last 100 epochs\n",
        "  if epoch == 200:\n",
        "    optimizer.lr *= 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b25I_me2_cGN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}