{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_17.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7rb4qBog8TWDx/aGEHugX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamestheengineer/data-science-from-scratch-Python/blob/master/Chapter_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXIJahnuYWRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "import math\n",
        "\n",
        "def entropy(class_probabilities: List[float]) -> float:\n",
        "  \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
        "  return sum(-p * math.log(p,2)\n",
        "              for p in class_probabilities\n",
        "             if p > 0) # ignore zero probabilities\n",
        "\n",
        "assert entropy([1.0]) == 0\n",
        "assert entropy([0.5, 0.5]) == 1\n",
        "assert 0.81 < entropy([0.25, 0.75]) < 0.82\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu7UOdk7Zo9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Any\n",
        "from collections import Counter\n",
        "\n",
        "def class_probabilities(labels: List[Any]) -> List[float]:\n",
        "  total_count = len(labels)\n",
        "  return [count / total_count\n",
        "          for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labels: List[Any]) -> float:\n",
        "  return entropy(class_probabilities(labels))\n",
        "\n",
        "assert data_entropy(['a']) == 0\n",
        "assert data_entropy([True, False]) == 1\n",
        "assert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])\n",
        "\n",
        "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
        "  \"\"\"Returns the entropy form this partition of data into subsets\"\"\"\n",
        "  total_count = sum(len(subset) for subset in subsets)\n",
        "\n",
        "  return sum(data_entropy(subset) * len(subset) / total_count\n",
        "             for subset in subsets)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkNIQ-UJebKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import NamedTuple, Optional\n",
        "\n",
        "class Candidate(NamedTuple):\n",
        "  level: str\n",
        "  lang: str\n",
        "  tweets: bool\n",
        "  phd: bool\n",
        "  did_well: Optional[bool] = None # allow unlabeled data\n",
        "\n",
        "           #  level     lang     tweets  phd  did_well\n",
        "inputs = [Candidate('Senior', 'Java',   False, False, False),\n",
        "          Candidate('Senior', 'Java',   False, True,  False),\n",
        "          Candidate('Mid',    'Python', False, False, True),\n",
        "          Candidate('Junior', 'Python', False, False, True),\n",
        "          Candidate('Junior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'R',      True,  True,  False),\n",
        "          Candidate('Mid',    'R',      True,  True,  True),\n",
        "          Candidate('Senior', 'Python', False, False, False),\n",
        "          Candidate('Senior', 'R',      True,  False, True),\n",
        "          Candidate('Junior', 'Python', True,  False, True),\n",
        "          Candidate('Senior', 'Python', True,  True,  True),\n",
        "          Candidate('Mid',    'Python', False, True,  True),\n",
        "          Candidate('Mid',    'Java',   True,  False, True),\n",
        "          Candidate('Junior', 'Python', False, True,  False)\n",
        "         ]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYsKvqr4ysVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Dict, TypeVar\n",
        "from collections import defaultdict\n",
        "\n",
        "T = TypeVar('T') # generic type for inputs\n",
        "\n",
        "def partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n",
        "  \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n",
        "  partitions: Dict[Any, List[T]] = defaultdict(list)\n",
        "  for input in inputs:\n",
        "    key = getattr(input, attribute) # value of the specified attribute\n",
        "    partitions[key].append(input) # add input to the correct partition\n",
        "  return partitions\n",
        "\n",
        "def partition_entropy_by(inputs: List[Any],\n",
        "                         attribute: str,\n",
        "                         label_attribute: str) -> float:\n",
        "  \"\"\"Compute the entropy correspionding to the given partition\"\"\"\n",
        "  # partitions consist of our inputs\n",
        "  partitions = partition_by(inputs, attribute)\n",
        "\n",
        "  # but partition_entropy needs just the class labels\n",
        "  labels = [[getattr(input, label_attribute) for input in partition]\n",
        "            for partition in partitions.values()]\n",
        "  \n",
        "  return partition_entropy(labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjXXWBO0Ich5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "afdc44b7-df97-4a90-fad1-4a40e0a465eb"
      },
      "source": [
        "for key in ['level', 'lang', 'tweets', 'phd']:\n",
        "  print(key, partition_entropy_by(inputs, key, 'did_well'))\n",
        "\n",
        "assert 0.69 < partition_entropy_by(inputs, 'level', 'did_well') < 0.70\n",
        "assert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well') < 0.87\n",
        "assert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well') < 0.79\n",
        "assert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well') < 0.90"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "level 0.6935361388961919\n",
            "lang 0.8601317128547441\n",
            "tweets 0.7884504573082896\n",
            "phd 0.8921589282623617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dxwQY58KkM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "senior_inputs = [input for input in inputs if input.level == 'Senior']\n",
        "\n",
        "assert 0.4 == partition_entropy_by(senior_inputs, 'lang', 'did_well')\n",
        "assert 0.0 == partition_entropy_by(senior_inputs, 'tweets', 'did_well')\n",
        "assert 0.95 < partition_entropy_by(senior_inputs, 'phd', 'did_well')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5NHnrkTMKzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}